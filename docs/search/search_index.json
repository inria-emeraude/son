{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SON: 3TC Audio Project @ INSA Lyon In this project, students program an embedded system (the Teensy 4.0: https://www.pjrc.com/store/teensy40.html ) for real-time audio signal processing applications. By doing so, they learn the basics of audio software architecture, audio signal processing in C++, and embedded system programming (C++). The idea is also to encourage students to develop a sense of independent work/entrepreneurship. The 2 weeks project period starts with a workshop on embedded real-time audio signal processing. During this workshop, students are walked through the architecture of a real-time audio DSP system (e.g., audio callback, buffering, sampling, etc.), and learn various basic techniques for audio signal processing (e.g., filters, oscillators, sound synthesis techniques, sound processing techniques, sound analysis techniques, etc.) taking a very practical approach. After this period, various project ideas are suggested to students. Projects can focus more on DSP or on the \"product/hardware\" aspect. Students work in groups of 2 on projects. The project period culminates in a final presentation taking the form of a poster/demo session where each group of students has a booth and can present its project, etc. Students are provided with plenty of fully-operational starter code/projects so that they have \"something that works\" right from the beginning. Instructors Romain Michon (Inria) Tanguy Risset (INSA Lyon) Clemens Wegener (Inria) Benjamin Qui\u00e9deville (Inria/GRAME) Resources Course GitHub Repository: https://github.com/inria-emeraude/son Teensy Audio Library: https://www.pjrc.com/teensy/td_libs_Audio.html A soldering room (next to radiocom room) The SON kit for a group of two students: A pair of headphones, one Teensy 4.0 with its companion audio shield, two buttons, two potentiometers, one breadboard, and a couple of resistors and jumper cables (do not loose these components!). Requirements Installing Teensyduino as explained in Lecture 1 . Course Overview Lecture 1: Course Introduction and Programming Environment Setup -- 02/02/2026 08h00-10h00 Lecture 2: Audio Signal Processing Fundamentals -- 02/02/2026 14h00-16h00 Lecture 3: Digital Audio Systems Architectures and Audio Callback -- 02/02/2026 16h00-18h00 Lecture 4: Hardware Control and Audio Codec Configuration -- 04/02/2026 08h00-10h00 Lecture 5: Introduction to Faust -- 04/02/2026 10h00-12h00 Lecture 6: Audio Processing Basics I -- 05/02/2026 08h00-10h00 Lab -- 05/02/2026 10h00-12h00 Lecture 7: Audio Processing Basics II -- 06/02/2026 08h00-10h00 Lecture 8: Faust on the Teensy and Advanced Control -- 06/02/2026 10h00-12h00 Project Validation Session -- 06/02/2026 16h00-18h00 Some Other Useful Things to Know Independent work on Projects -- 09/02/2026 - 27/02/2026 G1 G2 G3 G4 TP1 09/02/2026 -- 08h00-10h00 -- TD C 09/02/2026 -- 08h00-10h00 -- TD D 09/02/2026 -- 08h00-10h00 -- TD E 09/02/2026 -- 08h00-10h00 -- TD F TP2 09/02/2026 -- 14h00-16h00 -- TD E 09/02/2026 -- 14h00-16h00 -- TD C 09/02/2026 -- 14h00-16h00 -- TD D 09/02/2026 -- 14h00-16h00 -- TD F TP3 09/02/2026 -- 16h00-18h00 -- TD E 09/02/2026 -- 16h00-18h00 -- TD C 09/02/2026 -- 16h00-18h00 -- TD D 09/02/2026 -- 16h00-18h00 -- TD F TP4 11/02/2026 -- 08h00-10h00 -- TD E 11/02/2026 -- 08h00-10h00 -- TD C 11/02/2026 -- 08h00-10h00 -- TD D 11/02/2026 -- 08h00-10h00 -- TD F TP5 11/02/2026 -- 10h00-12h00 -- TD E 11/02/2026 -- 10h00-12h00 -- TD C 11/02/2026 -- 10h00-12h00 -- TD D 11/02/2026 -- 10h00-12h00 -- TD F TP6 11/02/2026 -- 14h00-16h00 -- TD C 11/02/2026 -- 14h00-16h00 -- TD D 11/02/2026 -- 14h00-16h00 -- TD F 11/02/2026 -- 14h00-16h00 -- TD E TP7 11/02/2026 -- 16h00-18h00 -- TD C 11/02/2026 -- 16h00-18h00 -- TD D 11/02/2026 -- 16h00-18h00 -- TD F 11/02/2026 -- 16h00-18h00 -- TD E TP8 12/02/2026 -- 08h00-10h00 -- TD F 12/02/2026 -- 08h00-10h00 -- TD E 12/02/2026 -- 08h00-10h00 -- TD D 12/02/2026 -- 08h00-10h00 -- TD C TP9 13/02/2026 -- 08h00-10h00 -- TD C 13/02/2026 -- 08h00-10h00 -- TD F 13/02/2026 -- 08h00-10h00 -- TD E 13/02/2026 -- 08h00-10h00 -- TD D TP10 13/02/2026 -- 10h00-12h00 -- TD C 13/02/2026 -- 10h00-12h00 -- TD F 13/02/2026 -- 10h00-12h00 -- TD E 13/02/2026 -- 10h00-12h00 -- TD D MLS/TP11 13/02/2026 -- 14h00-16h00 -- TD E 13/02/2026 -- 14h00-16h00 -- TD C 13/02/2026 -- 14h00-16h00 -- TD D 13/02/2026 -- 14h00-16h00 -- TD F MLS/TP12 13/02/2026 -- 16h00-18h00 -- TD E 13/02/2026 -- 16h00-18h00 -- TD C 13/02/2026 -- 16h00-18h00 -- TD D 13/02/2026 -- 16h00-18h00 -- TD F TP13 23/02/2026 -- 08h00-10h00 -- TD E 23/02/2026 -- 08h00-10h00 -- TD D 23/02/2026 -- 08h00-10h00 -- TD F 23/02/2026 -- 08h00-10h00 -- TD C TP14 24/02/2026 -- 14h00-16h00 -- TD F 24/02/2026 -- 14h00-16h00 -- TD C 24/02/2026 -- 14h00-16h00 -- TD E 24/02/2026 -- 14h00-16h00 -- TD D TP15 25/02/2026 -- 08h00-10h00 -- TD TODO 25/02/2026 -- 08h00-10h00 -- TD TODO 25/02/2026 -- 08h00-10h00 -- TD TODO 25/02/2026 -- 08h00-10h00 -- TD TODO TP16 25/02/2026 -- 10h00-12h00 -- TD TODO 25/02/2026 -- 10h00-12h00 -- TD TODO 25/02/2026 -- 10h00-12h00 -- TD TODO 25/02/2026 -- 10h00-12h00 -- TD TODO TP17 25/02/2026 -- 14h00-16h00 -- TD TODO 25/02/2026 -- 14h00-16h00 -- TD TODO 25/02/2026 -- 14h00-16h00 -- TD TODO 25/02/2026 -- 14h00-16h00 -- TD TODO TP18 25/02/2026 -- 16h00-18h00 -- TD TODO 25/02/2026 -- 16h00-18h00 -- TD TODO 25/02/2026 -- 16h00-18h00 -- TD TODO 25/02/2026 -- 16h00-18h00 -- TD TODO TP19 26/02/2026 -- 08h00-10h00 -- TD TODO 26/02/2026 -- 08h00-10h00 -- TD TODO 26/02/2026 -- 08h00-10h00 -- TD TODO 26/02/2026 -- 08h00-10h00 -- TD TODO Final Presentations (G1 + G2) -- 27/02/2026 08h00-12h00 Final Presentations (G3 + G4) -- 27/02/2025 14h00-18h00","title":"SON: 3TC Audio Project @ INSA Lyon"},{"location":"#son-3tc-audio-project-insa-lyon","text":"In this project, students program an embedded system (the Teensy 4.0: https://www.pjrc.com/store/teensy40.html ) for real-time audio signal processing applications. By doing so, they learn the basics of audio software architecture, audio signal processing in C++, and embedded system programming (C++). The idea is also to encourage students to develop a sense of independent work/entrepreneurship. The 2 weeks project period starts with a workshop on embedded real-time audio signal processing. During this workshop, students are walked through the architecture of a real-time audio DSP system (e.g., audio callback, buffering, sampling, etc.), and learn various basic techniques for audio signal processing (e.g., filters, oscillators, sound synthesis techniques, sound processing techniques, sound analysis techniques, etc.) taking a very practical approach. After this period, various project ideas are suggested to students. Projects can focus more on DSP or on the \"product/hardware\" aspect. Students work in groups of 2 on projects. The project period culminates in a final presentation taking the form of a poster/demo session where each group of students has a booth and can present its project, etc. Students are provided with plenty of fully-operational starter code/projects so that they have \"something that works\" right from the beginning.","title":"SON: 3TC Audio Project @ INSA Lyon"},{"location":"#instructors","text":"Romain Michon (Inria) Tanguy Risset (INSA Lyon) Clemens Wegener (Inria) Benjamin Qui\u00e9deville (Inria/GRAME)","title":"Instructors"},{"location":"#resources","text":"Course GitHub Repository: https://github.com/inria-emeraude/son Teensy Audio Library: https://www.pjrc.com/teensy/td_libs_Audio.html A soldering room (next to radiocom room) The SON kit for a group of two students: A pair of headphones, one Teensy 4.0 with its companion audio shield, two buttons, two potentiometers, one breadboard, and a couple of resistors and jumper cables (do not loose these components!).","title":"Resources"},{"location":"#requirements","text":"Installing Teensyduino as explained in Lecture 1 .","title":"Requirements"},{"location":"#course-overview","text":"Lecture 1: Course Introduction and Programming Environment Setup -- 02/02/2026 08h00-10h00 Lecture 2: Audio Signal Processing Fundamentals -- 02/02/2026 14h00-16h00 Lecture 3: Digital Audio Systems Architectures and Audio Callback -- 02/02/2026 16h00-18h00 Lecture 4: Hardware Control and Audio Codec Configuration -- 04/02/2026 08h00-10h00 Lecture 5: Introduction to Faust -- 04/02/2026 10h00-12h00 Lecture 6: Audio Processing Basics I -- 05/02/2026 08h00-10h00 Lab -- 05/02/2026 10h00-12h00 Lecture 7: Audio Processing Basics II -- 06/02/2026 08h00-10h00 Lecture 8: Faust on the Teensy and Advanced Control -- 06/02/2026 10h00-12h00 Project Validation Session -- 06/02/2026 16h00-18h00 Some Other Useful Things to Know Independent work on Projects -- 09/02/2026 - 27/02/2026 G1 G2 G3 G4 TP1 09/02/2026 -- 08h00-10h00 -- TD C 09/02/2026 -- 08h00-10h00 -- TD D 09/02/2026 -- 08h00-10h00 -- TD E 09/02/2026 -- 08h00-10h00 -- TD F TP2 09/02/2026 -- 14h00-16h00 -- TD E 09/02/2026 -- 14h00-16h00 -- TD C 09/02/2026 -- 14h00-16h00 -- TD D 09/02/2026 -- 14h00-16h00 -- TD F TP3 09/02/2026 -- 16h00-18h00 -- TD E 09/02/2026 -- 16h00-18h00 -- TD C 09/02/2026 -- 16h00-18h00 -- TD D 09/02/2026 -- 16h00-18h00 -- TD F TP4 11/02/2026 -- 08h00-10h00 -- TD E 11/02/2026 -- 08h00-10h00 -- TD C 11/02/2026 -- 08h00-10h00 -- TD D 11/02/2026 -- 08h00-10h00 -- TD F TP5 11/02/2026 -- 10h00-12h00 -- TD E 11/02/2026 -- 10h00-12h00 -- TD C 11/02/2026 -- 10h00-12h00 -- TD D 11/02/2026 -- 10h00-12h00 -- TD F TP6 11/02/2026 -- 14h00-16h00 -- TD C 11/02/2026 -- 14h00-16h00 -- TD D 11/02/2026 -- 14h00-16h00 -- TD F 11/02/2026 -- 14h00-16h00 -- TD E TP7 11/02/2026 -- 16h00-18h00 -- TD C 11/02/2026 -- 16h00-18h00 -- TD D 11/02/2026 -- 16h00-18h00 -- TD F 11/02/2026 -- 16h00-18h00 -- TD E TP8 12/02/2026 -- 08h00-10h00 -- TD F 12/02/2026 -- 08h00-10h00 -- TD E 12/02/2026 -- 08h00-10h00 -- TD D 12/02/2026 -- 08h00-10h00 -- TD C TP9 13/02/2026 -- 08h00-10h00 -- TD C 13/02/2026 -- 08h00-10h00 -- TD F 13/02/2026 -- 08h00-10h00 -- TD E 13/02/2026 -- 08h00-10h00 -- TD D TP10 13/02/2026 -- 10h00-12h00 -- TD C 13/02/2026 -- 10h00-12h00 -- TD F 13/02/2026 -- 10h00-12h00 -- TD E 13/02/2026 -- 10h00-12h00 -- TD D MLS/TP11 13/02/2026 -- 14h00-16h00 -- TD E 13/02/2026 -- 14h00-16h00 -- TD C 13/02/2026 -- 14h00-16h00 -- TD D 13/02/2026 -- 14h00-16h00 -- TD F MLS/TP12 13/02/2026 -- 16h00-18h00 -- TD E 13/02/2026 -- 16h00-18h00 -- TD C 13/02/2026 -- 16h00-18h00 -- TD D 13/02/2026 -- 16h00-18h00 -- TD F TP13 23/02/2026 -- 08h00-10h00 -- TD E 23/02/2026 -- 08h00-10h00 -- TD D 23/02/2026 -- 08h00-10h00 -- TD F 23/02/2026 -- 08h00-10h00 -- TD C TP14 24/02/2026 -- 14h00-16h00 -- TD F 24/02/2026 -- 14h00-16h00 -- TD C 24/02/2026 -- 14h00-16h00 -- TD E 24/02/2026 -- 14h00-16h00 -- TD D TP15 25/02/2026 -- 08h00-10h00 -- TD TODO 25/02/2026 -- 08h00-10h00 -- TD TODO 25/02/2026 -- 08h00-10h00 -- TD TODO 25/02/2026 -- 08h00-10h00 -- TD TODO TP16 25/02/2026 -- 10h00-12h00 -- TD TODO 25/02/2026 -- 10h00-12h00 -- TD TODO 25/02/2026 -- 10h00-12h00 -- TD TODO 25/02/2026 -- 10h00-12h00 -- TD TODO TP17 25/02/2026 -- 14h00-16h00 -- TD TODO 25/02/2026 -- 14h00-16h00 -- TD TODO 25/02/2026 -- 14h00-16h00 -- TD TODO 25/02/2026 -- 14h00-16h00 -- TD TODO TP18 25/02/2026 -- 16h00-18h00 -- TD TODO 25/02/2026 -- 16h00-18h00 -- TD TODO 25/02/2026 -- 16h00-18h00 -- TD TODO 25/02/2026 -- 16h00-18h00 -- TD TODO TP19 26/02/2026 -- 08h00-10h00 -- TD TODO 26/02/2026 -- 08h00-10h00 -- TD TODO 26/02/2026 -- 08h00-10h00 -- TD TODO 26/02/2026 -- 08h00-10h00 -- TD TODO Final Presentations (G1 + G2) -- 27/02/2026 08h00-12h00 Final Presentations (G3 + G4) -- 27/02/2025 14h00-18h00","title":"Course Overview"},{"location":"projects/","text":"Final Projects Final projects constitute the heart of this course: students are 100% evaluated on projects. While you should feel free to use any of the code studied during class, found on the web, or generated using tools such as Faust , the following criterion should be taken into account: Doing what you were asked to do is good, but doing more is better :); Novelty, creativity, and initiatives are crucial: consider your project as a potential startup idea, think about how it can be better than what already exists. Along the same lines, try to have a \"product\" as finished and as polished as possible; Consider the research and scientific aspect of what you're doing: most project ideas will involve some bibliographical work before getting your hands dirty, this will help you get a sense of what has already been done by others and it will save you time when readying your poster. Evaluation Projects will be evaluated through: A poster (pdf) presenting your project; A demo video; A demo of your project which will be featured during a demo session taking place on 28/02/2025. During this session, students will use the poster of their project directly on their computer as a support for providing additional information to the \"public\" (i.e., the other students and the instructors/evaluators). The quality and the presentation of the poster and of the webpage will be taken into account towards your final grade. As for the project itself, you will be evaluated on its: robustness/stability; interface; novelty; rigorousness; overall quality; potential link to research. Final Project Ideas Auto-Tune Auto-tune ( https://en.wikipedia.org/wiki/Auto-Tune ) is a sound effect which automatically corrects the pitch of a sound by quantizing it to the nearest note of the chromatic scale. The goal of this project is to make an \"autotune box/audio processor\" based on the Teensy. In addition to figuring out how to implement the algorithm (which should involve pitch shifting and frequency tracking), you should think about the interface that your project will have (which parameters can be controlled?, etc.). Vocoder A Vocoder https://en.wikipedia.org/wiki/Vocoder is an audio effect which replaces the natural sound of human voice with an artificial/synthesized one by preserving intelligibility of speech. It can be used to create a \"robot/ghost voice.\" The goal of this project is to make a \"vocoder box/audio processor\" based on the Teensy. In addition to figuring out how to implement the algorithm (which should involve pitch shifting and frequency tracking), you should think about the interface that your project will have (which parameters can be controlled?, etc.). Pitch-Shifting Module Pitch-Shifting consists in transposing (up or down) the sound of an input source so that it sounds lower or higher. Pitch shifting can potentially be implemented using 2 different techniques: (i) using an FFT to transpose the spectrum and then an inverse FFT to put it back in the time domain, (ii) using the doppler effect produced by changing the length of delay lines in real-time. Hence, the goal of this project is to make a \"pitch shifting box/audio processor\" based on the Teensy. In addition to figuring out how to implement the algorithm (which should involve pitch shifting and frequency tracking), you should think about the interface that your project will have (which parameters can be controlled?, etc.). Hearing Aid At the most fundamental level, hearing aids just implement a bunch of equalization filters (see Peak Equalizers ) increasing the volume of some bands in the spectrum of a sound (the ones that an individual can't hear well anymore). The goal of this project is to implement an hearing aid prototype based on the Teensy. If time allows it, you should try to measure the audiogram of someone with hearing loss to test your system in a real-world context. Cochlear Implant Simulator Cochlear implants ( https://en.wikipedia.org/wiki/Cochlear_implant ) allow many people who were condemned to be deaf their entire life to now hear some sound and even speech if they're well trained. This paper: https://embaudio.grame.fr/cochlear.pdf describes precisely how a cochlear implant simulator can be implemented using DSP. The goal of this project is to create a sound processing box simulating the sound of a cochlear implant. For that, you will probably need some of the filters described here . Sonification System Data sonification is a trending method for analyzing data in real-time. The brain stethoscope ( https://youtu.be/pnGzVW_7cfM ) is a good example of that. Choose a sensor technology and develop a sonification technology around it. Superman Hearing Try to make your own version of Facebook's \"superman hearing\" system ( https://about.fb.com/news/2020/09/facebook-reality-labs-research-future-of-audio/ ). Guitar Pedal Effect There exist dozens of guitar pedal effects: https://en.wikipedia.org/wiki/Effects_unit . The goal of this project is to make a guitar pedal \"stompbox.\" Don't forget to think about the interface and the shape of the final product. Sound Synthesis Module Modular synthesizers ( https://en.wikipedia.org/wiki/Modular_synthesizer ) became very popular in recent years. Each hardware module implements and sound synthesis or processing technique and offers an interface made of buttons, knobs, etc. The goal of this project is to implement your own sound synthesis module based on a Teensy. If you own a modular synth yourself, think about how your module could integrate to your toolchain, etc. EP (Vinyl) Simulator Love the sound of EPs ( https://en.wikipedia.org/wiki/Extended_play )? Implement a system that plugs to your living room sound system and makes the sound of any device you plug to it sound like an EP. For that, you will first have to document yourself on the impact on sound of this analog technology. There exists a couple of scientific papers that describe this fairly well. Make sure that you think about the interface that your system will have and about the form that your final product will take, etc. Feedback Cancellation Microphone Feedback happens when a microphone and speaker implement a feedback loop. This results in a very annoying sound which sound engineers try to avoid as much as possible during concerts. Try to make a microphone carrying out real-time audio DSP to limit feedback. There exist many scientific papers on this topic which means that you will first have to document yourself on this topic. Augmented Speaker Make a speaker processing its audio input in real time to add audio effects, improve the quality of sound, etc. The goal of this project is to focus more on the product design aspect to try to design an appealing object. Think about the interface that system could have (maybe accessible through a smartphone? Etc.). Musical Instrument The goal of this project is to make musical instrument based on a Teensy. The domain of New Interfaces for Musical Expression (NIME): https://www.nime.org/ has been booming in recent years and hundreds of musical instruments have been created. Feel free to get more inspiration here: https://ccrma.stanford.edu/courses/250a-spring-2023 ! MIDI-Controlled Polyphonic Synthesizer Build a polyphonic (multiple notes can be played at the same time) synthesizer based on a USB MIDI keyboard (using the Teensy USB MIDI Library ). The keyboard should connect directly to the Teensy through the USB port. The synth should have multiple parameters to control through MIDI CCs. For this project, you'll have to find a way to externally power your Teensy since the USB port will be used for connecting the MIDI keyboard, etc. Granular Synthesizer Granular synthesis https://en.wikipedia.org/wiki/Granular_synthesis consists in looping a short sound sample to generate sound textures with it. The goal of this project is to make a granular synthesis module based on a Teensy. You should think about the interface that your system will have (i.e., which parameter to control, etc.). Note that since you will have to record audio samples on the Teensy, you will likely have to use an SD card for this because of the limited amount of memory available on the Teensy. DAFx DAFx is THE conference on digital audio effects: https://www.dafx.de/ . The goal of this project is to pickup any paper in the DAFx literature and implement it on the Teensy. Any Other Idea... Feel free to work on anything else but talk to us first :).","title":"Projects"},{"location":"projects/#final-projects","text":"Final projects constitute the heart of this course: students are 100% evaluated on projects. While you should feel free to use any of the code studied during class, found on the web, or generated using tools such as Faust , the following criterion should be taken into account: Doing what you were asked to do is good, but doing more is better :); Novelty, creativity, and initiatives are crucial: consider your project as a potential startup idea, think about how it can be better than what already exists. Along the same lines, try to have a \"product\" as finished and as polished as possible; Consider the research and scientific aspect of what you're doing: most project ideas will involve some bibliographical work before getting your hands dirty, this will help you get a sense of what has already been done by others and it will save you time when readying your poster.","title":"Final Projects"},{"location":"projects/#evaluation","text":"Projects will be evaluated through: A poster (pdf) presenting your project; A demo video; A demo of your project which will be featured during a demo session taking place on 28/02/2025. During this session, students will use the poster of their project directly on their computer as a support for providing additional information to the \"public\" (i.e., the other students and the instructors/evaluators). The quality and the presentation of the poster and of the webpage will be taken into account towards your final grade. As for the project itself, you will be evaluated on its: robustness/stability; interface; novelty; rigorousness; overall quality; potential link to research.","title":"Evaluation"},{"location":"projects/#final-project-ideas","text":"","title":"Final Project Ideas"},{"location":"projects/#auto-tune","text":"Auto-tune ( https://en.wikipedia.org/wiki/Auto-Tune ) is a sound effect which automatically corrects the pitch of a sound by quantizing it to the nearest note of the chromatic scale. The goal of this project is to make an \"autotune box/audio processor\" based on the Teensy. In addition to figuring out how to implement the algorithm (which should involve pitch shifting and frequency tracking), you should think about the interface that your project will have (which parameters can be controlled?, etc.).","title":"Auto-Tune"},{"location":"projects/#vocoder","text":"A Vocoder https://en.wikipedia.org/wiki/Vocoder is an audio effect which replaces the natural sound of human voice with an artificial/synthesized one by preserving intelligibility of speech. It can be used to create a \"robot/ghost voice.\" The goal of this project is to make a \"vocoder box/audio processor\" based on the Teensy. In addition to figuring out how to implement the algorithm (which should involve pitch shifting and frequency tracking), you should think about the interface that your project will have (which parameters can be controlled?, etc.).","title":"Vocoder"},{"location":"projects/#pitch-shifting-module","text":"Pitch-Shifting consists in transposing (up or down) the sound of an input source so that it sounds lower or higher. Pitch shifting can potentially be implemented using 2 different techniques: (i) using an FFT to transpose the spectrum and then an inverse FFT to put it back in the time domain, (ii) using the doppler effect produced by changing the length of delay lines in real-time. Hence, the goal of this project is to make a \"pitch shifting box/audio processor\" based on the Teensy. In addition to figuring out how to implement the algorithm (which should involve pitch shifting and frequency tracking), you should think about the interface that your project will have (which parameters can be controlled?, etc.).","title":"Pitch-Shifting Module"},{"location":"projects/#hearing-aid","text":"At the most fundamental level, hearing aids just implement a bunch of equalization filters (see Peak Equalizers ) increasing the volume of some bands in the spectrum of a sound (the ones that an individual can't hear well anymore). The goal of this project is to implement an hearing aid prototype based on the Teensy. If time allows it, you should try to measure the audiogram of someone with hearing loss to test your system in a real-world context.","title":"Hearing Aid"},{"location":"projects/#cochlear-implant-simulator","text":"Cochlear implants ( https://en.wikipedia.org/wiki/Cochlear_implant ) allow many people who were condemned to be deaf their entire life to now hear some sound and even speech if they're well trained. This paper: https://embaudio.grame.fr/cochlear.pdf describes precisely how a cochlear implant simulator can be implemented using DSP. The goal of this project is to create a sound processing box simulating the sound of a cochlear implant. For that, you will probably need some of the filters described here .","title":"Cochlear Implant Simulator"},{"location":"projects/#sonification-system","text":"Data sonification is a trending method for analyzing data in real-time. The brain stethoscope ( https://youtu.be/pnGzVW_7cfM ) is a good example of that. Choose a sensor technology and develop a sonification technology around it.","title":"Sonification System"},{"location":"projects/#superman-hearing","text":"Try to make your own version of Facebook's \"superman hearing\" system ( https://about.fb.com/news/2020/09/facebook-reality-labs-research-future-of-audio/ ).","title":"Superman Hearing"},{"location":"projects/#guitar-pedal-effect","text":"There exist dozens of guitar pedal effects: https://en.wikipedia.org/wiki/Effects_unit . The goal of this project is to make a guitar pedal \"stompbox.\" Don't forget to think about the interface and the shape of the final product.","title":"Guitar Pedal Effect"},{"location":"projects/#sound-synthesis-module","text":"Modular synthesizers ( https://en.wikipedia.org/wiki/Modular_synthesizer ) became very popular in recent years. Each hardware module implements and sound synthesis or processing technique and offers an interface made of buttons, knobs, etc. The goal of this project is to implement your own sound synthesis module based on a Teensy. If you own a modular synth yourself, think about how your module could integrate to your toolchain, etc.","title":"Sound Synthesis Module"},{"location":"projects/#ep-vinyl-simulator","text":"Love the sound of EPs ( https://en.wikipedia.org/wiki/Extended_play )? Implement a system that plugs to your living room sound system and makes the sound of any device you plug to it sound like an EP. For that, you will first have to document yourself on the impact on sound of this analog technology. There exists a couple of scientific papers that describe this fairly well. Make sure that you think about the interface that your system will have and about the form that your final product will take, etc.","title":"EP (Vinyl) Simulator"},{"location":"projects/#feedback-cancellation-microphone","text":"Feedback happens when a microphone and speaker implement a feedback loop. This results in a very annoying sound which sound engineers try to avoid as much as possible during concerts. Try to make a microphone carrying out real-time audio DSP to limit feedback. There exist many scientific papers on this topic which means that you will first have to document yourself on this topic.","title":"Feedback Cancellation Microphone"},{"location":"projects/#augmented-speaker","text":"Make a speaker processing its audio input in real time to add audio effects, improve the quality of sound, etc. The goal of this project is to focus more on the product design aspect to try to design an appealing object. Think about the interface that system could have (maybe accessible through a smartphone? Etc.).","title":"Augmented Speaker"},{"location":"projects/#musical-instrument","text":"The goal of this project is to make musical instrument based on a Teensy. The domain of New Interfaces for Musical Expression (NIME): https://www.nime.org/ has been booming in recent years and hundreds of musical instruments have been created. Feel free to get more inspiration here: https://ccrma.stanford.edu/courses/250a-spring-2023 !","title":"Musical Instrument"},{"location":"projects/#midi-controlled-polyphonic-synthesizer","text":"Build a polyphonic (multiple notes can be played at the same time) synthesizer based on a USB MIDI keyboard (using the Teensy USB MIDI Library ). The keyboard should connect directly to the Teensy through the USB port. The synth should have multiple parameters to control through MIDI CCs. For this project, you'll have to find a way to externally power your Teensy since the USB port will be used for connecting the MIDI keyboard, etc.","title":"MIDI-Controlled Polyphonic Synthesizer"},{"location":"projects/#granular-synthesizer","text":"Granular synthesis https://en.wikipedia.org/wiki/Granular_synthesis consists in looping a short sound sample to generate sound textures with it. The goal of this project is to make a granular synthesis module based on a Teensy. You should think about the interface that your system will have (i.e., which parameter to control, etc.). Note that since you will have to record audio samples on the Teensy, you will likely have to use an SD card for this because of the limited amount of memory available on the Teensy.","title":"Granular Synthesizer"},{"location":"projects/#dafx","text":"DAFx is THE conference on digital audio effects: https://www.dafx.de/ . The goal of this project is to pickup any paper in the DAFx literature and implement it on the Teensy.","title":"DAFx"},{"location":"projects/#any-other-idea","text":"Feel free to work on anything else but talk to us first :).","title":"Any Other Idea..."},{"location":"lectures/audio-dsp/","text":"Audio Signal Processing Fundamentals The goal of this lecture is to provide an overview of the basics of digital audio. Analog Audio Signals Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen . This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer The Discovery of Digital Audio Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is a variation of tension in time in an electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia ) ADC and DAC In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.). Human Hearing Range and Sampling Rate One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz. Sampling Theorem Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here . Aliasing Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening. Bit Depth, Dynamic Range and Signal-to-Noise Ratio Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here . Range of Audio Samples Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signals to this range to prevent warping and will hence result in clipping if exceeded. First Synthesized Sound on a Digital Computer While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":" 2: Audio Signal Processing Fundamentals "},{"location":"lectures/audio-dsp/#audio-signal-processing-fundamentals","text":"The goal of this lecture is to provide an overview of the basics of digital audio.","title":"Audio Signal Processing Fundamentals"},{"location":"lectures/audio-dsp/#analog-audio-signals","text":"Before the advent of digital audio, most audio systems/technologies were analog. An analog audio signal can take different forms: it can be electric (e.g., transmitted through an electric wire and stored on a magnetic tape) or mechanical (e.g., transmitted through the air as standing waves and stored on a vinyl disc). Acoustical mechanical waves can be converted into an electric signal using a microphone. Conversely, an electric audio signal can be converted into mechanical acoustical waves using a speaker. In nature, sounds almost always originate from a mechanical source. However, in the 20th century, many musicians, composers and engineers experimented with the production of sound from an electrical source. One of the pioneer in this field was Karlheinz Stockhausen . This lead to analog and modular synthesizers which are very popular among Croix-Roussian hipsters these days. A modular analog synthesizer","title":"Analog Audio Signals"},{"location":"lectures/audio-dsp/#the-discovery-of-digital-audio","text":"Sampling theory dates back from the beginning of the 20th century with initial work by Harry Nyquist and was theorized in the 1930s by Claude Shannon to become the Nyquist-Shannon sampling theorem. Carrying sampling in the field of audio is relatively simple: voltage measurements are carried out at regular intervals of time on an analog electrical signal. Each individual acquired value is called a \"sample\" and can be stored on a computer. Hence, while an analog electric audio signal is a variation of tension in time in an electric cable, a digital audio signal is just series of samples (values) in time as well. Signal sampling representation. The continuous signal is represented with a green colored line while the discrete samples are indicated by the blue vertical lines. (source: Wikipedia )","title":"The Discovery of Digital Audio"},{"location":"lectures/audio-dsp/#adc-and-dac","text":"In the field of audio, an ADC (Analog to Digital Converter) is a hardware component that can be used to discretize (sample) an electrical analog audio signal. The reverse operation is carried out using a DAC (Digital to Analog Converter). In most systems, the ADC and the DAC are hosted in the same piece of hardware (e.g., audio codec, audio interface, etc.).","title":"ADC and DAC"},{"location":"lectures/audio-dsp/#human-hearing-range-and-sampling-rate","text":"One of the main factor to consider when sampling an audio signal is the human hearing range. In theory, humans can hear any sound between 20 and 20000 Hz. In practice, our ability to perceive high frequencies decays over time and is affected by environmental factors (e.g., if we're exposed to sound with high volume, if we contract some diseases such as hear infections, etc.). By the age of 30, most adults can't hear frequencies over 17 kHz. When sampling an audio signal, the number of samples per second also known as the sampling rate (noted fs ) will determine the highest frequency than can be sampled by the system. The rule is very simple: the highest frequency that can be sampled is half the sampling rate. Hence, in order to sample a frequency of 20 kHz, the sampling rate of the system must be at least 40 kHz which corresponds to 40000 values (samples) per second. The highest frequency that can be sampled is also known as the \" Nyquist Frequency \" ( fn ): fn=\\frac{fs}{2} The standard for modern audio systems is to use a sampling rate of 48 kHz. fs is 44.1 kHz on compact discs (CDs) and many home and recording studios use a sampling rate of 96 or 192 kHz.","title":"Human Hearing Range and Sampling Rate"},{"location":"lectures/audio-dsp/#sampling-theorem","text":"Let x(t) denote any continuous-time signal having a continuous Fourier transform : X(j\\omega) \\triangleq \\int_{-\\infty}^{\\infty}x(t)e^{-j \\omega t}dt Let x_d(n) \\triangleq x(nT), \\quad n=\\dots,-2,-1,0,1,2,\\dots, denote the samples of x(t) at uniform intervals of T seconds. Then x(t) can be exactly reconstructed from its samples x_d(n) if X(j\\omega)=0 for all \\vert\\omega\\vert\\geq\\pi/T . In other words, any frequency (harmonics) between 0 Hz and the Nyquist frequency can be exactly reconstructed without loosing any information. That also means that if the Nyquist frequency is above the upper threshold of the human hearing range (e.g., 20 kHz), a digitized signal should sound exactly the same as its analog counterpart from a perceptual standpoint. Additional proofs about the sampling theorem can be found on Julius Smith's website here .","title":"Sampling Theorem"},{"location":"lectures/audio-dsp/#aliasing","text":"Aliasing is a well known phenomenon in the field of video: In audio, aliasing happens when a digital signal contains frequencies above the Nyquist frequency. In that case, they are not sampled at the right frequency and they are wrapped. Hence, for all frequency fo above fn , the sampled frequency f will be: f = fn - (fo-fn) with fn = \\frac{fs}{2} Aliasing is typically prevented by filtering an analog signal before it is discretized by removing all frequency above fn . Aliasing can also be obtained when synthesizing a broadband signal on a computer (e.g., a sawtooth wave). It is the software engineer's role to prevent this from happening.","title":"Aliasing"},{"location":"lectures/audio-dsp/#bit-depth-dynamic-range-and-signal-to-noise-ratio","text":"Beside sampling rate, the other parameter of sampling is the bit depth of audio samples. Audio is typically recorded at 8, 16 (the standard for CDs), or 24 bits (and 32 bits in some rarer cases). A higher bit depth means a more accurate precision for a given audio sample. This impacts directly the dynamic range and the signal-to-noise (SNR) ratio of a digital signal. In other words, a smaller bit depth will mean more noise in the signal, etc. Additional information about this topic can be found here .","title":"Bit Depth, Dynamic Range and Signal-to-Noise Ratio"},{"location":"lectures/audio-dsp/#range-of-audio-samples","text":"Audio samples can be coded in many different ways depending on the context. Some low-level systems use fixed-point numbers (i.e., integers) for efficiency. In that case, the range of the signal will be determined by the data type. For example, if audio samples are coded on 16 bits unsigned integers, the range of the signal will be 0 to 2^{16} - 1 (or 65535). At the hardware level (e.g., ADC/DAC), audio samples are almost exclusively coded on integers. On the other hand, fixed points are relatively hard to deal with at the software level when it comes to implementing DSP algorithms. In that case, it is much more convenient to use decimal numbers (i.e., floating points). The established standard in audio is that audio signals coded on decimal numbers always have the following range: {-1;1}. While this range can be exceeded within an algorithm without any consequences, the inputs and outputs of a DSP block must always be constrained between -1 and 1. Most systems will clip audio signals to this range to prevent warping and will hence result in clipping if exceeded.","title":"Range of Audio Samples"},{"location":"lectures/audio-dsp/#first-synthesized-sound-on-a-digital-computer","text":"While Shanon and Nyquist theorized sampling in the 1930s, it's only in 1958 that a sound was synthesized for the first time on a computer by Max Mathews at Bell Labs, giving birth a few years later to the first song synthesized (and sung) by a computer: This was by the way reused by Stanley Kubrick in one of his famous movie as HAL the computer is slowly dying as it's being unplugged: These technologies were then extensively exploited until today both for musical applications and in the industry at large.","title":"First Synthesized Sound on a Digital Computer"},{"location":"lectures/audio-sys/","text":"Digital Audio Systems Architectures and Audio Callback By the end of this lecture, you should be able to produce sound with your Teensy and have a basic understanding of the software and hardware architecture of embedded audio systems. Basic Architecture of a Digital Audio System All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system. Architecture of Embedded Audio Systems Such as the Teensy In embedded audio systems, the component implementing the audio ADC and DAC is called an \"Audio Codec.\" This name is slightly ambiguous because it is also used in the context of audio compression (e.g., mp3) to designate a totally different concept. In the case of the Teensy kits that are provided to you as part of this class, the audio codec we use is an SGTL5000. It is mounted on a shield/sister board that has the same form factor as the Teensy. Audio samples are sent and received between the Cortex M7 and the audio codec using the i2s protocol. As a microcontroller, the Cortex M7 has its own analog inputs which can be used to retrieve sensor datas (e.g., potentiometers, etc.). These analog inputs cannot be used for audio because of their limited precision and sampling rate. We'll briefly show in this lecture about control how these analog inputs can be used to use sensors to control audio algorithms running on the Teensy. Teensy and Audio Shield Overview Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result on the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function typically takes the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the Teensy can achieve much lower latencies than regular computers because of their lightness. Hence, the block size of your Teensy can be as small as 8! First Audio Program on the Teensy: crazy-sine The course repository hosts an example containing a program synthesizing a sine wave on the Teensy and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the update method and take the following shape: #define MULT_16 32767 void MyDsp::update(void) { audio_block_t* outBlock[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; channel++) { outBlock[channel] = allocate(); if (outBlock[channel]) { for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { float currentSample = echo.tick(sine.tick())*0.5; currentSample = max(-1,min(1,currentSample)); int16_t val = currentSample*MULT_16; outBlock[channel]->data[i] = val; } transmit(outBlock[channel], channel); release(outBlock[channel]); } } } The update method is called every time a new audio buffer is needed by the system. A new audio buffer audioBlock containing AUDIO_OUTPUTS channels is first created. For every audio channel, memory is allocated and a full block of samples is computed. Individual samples resulting from computing a sine wave through an echo ( echo and sine are defined in the libraries folder and implement an echo and a sine wave oscillator, respectively) are stored in currentSample . currentSample is a floating point number whose range is {-1;1}. This is a standard in the world of digital audio, hence, a signal actually ranging between {-1;1} will correspond to the \"loudest\" sound that can be played on a given system. max(-1,min(1,currentSample)); ensures that currentSample doesn't exceed this range. AUDIO_BLOCK_SAMPLES corresponds to the block size (256 samples by default on the Teensy, but this value can potentially be adjusted). The values contained in currentSample (between -1 and 1) must be converted to 16 bits signed integers (to ensure compatibility with the rest of the Teensy audio library). For that, we just have to multiply currentSample by 2^{16-1} (if we were looking at unsigned integers, which can happen on some system, we would multiply currentSample by 2^{16} ). Note that currentSample is multiplied by 0.5 to control the output gain of the system here (we'll see later in this class that echos tend to add energy to the system hence we must limit the gain of the output signal to prevent potential saturation). Once a full block has been computed, it is transmitted to the rest of the system using the transmit function. Once this is done, the memory that was allocated for the audio block is freed using the release function. The update method is called over and over until the Teensy is powered out. C++ Sine Wave Oscillator Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp , which is used in crazy-sine is a good example of that. It uses SineTable.cpp which pre-computes a sine table: table = new float[size]; for(int i=0; i<size; i++){ table[i] = std::sin(i*2.0*PI/size); } and then makes it accessible through the tick (compute) method: float SineTable::tick(int index){ return table[index%tableSize]; } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The sine table is then read with a \"phasor.\" A phasor produces a ramp signal which is reset at a certain frequency. It can also be seen as a sawtooth wave. Phasor.cpp is used for that purpose and its tick method is defined as: float Phasor::tick(){ float currentSample = phasor; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } It hence ramps from 0 to 1 at a given frequency. The phasor object in Sine.cpp is used to read the sine table by adjusting the range of its output: float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(index)*gain; } Exercises Looping Through a Small Tune In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your Teensy. Hint: For that, you'll probably have to replace the myDsp.setFreq(random(50,1000)); line of of crazy-sine.ino by something else. Solution: In crazy-sine.ino : #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); float mtof(float note){ return pow(2.0,(note-69.0)/12.0)*440.0; } int tune[] = {62,78,65,67,69}; int cnt = 0; void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { myDsp.setFreq(mtof(tune[cnt])); cnt = (cnt+1)%5; delay(500); } Basic Additive Synthesis One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product. Solution: In Sine.cpp : float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; int index2 = (int) (index*1.5)%SINE_TABLE_SIZE; return (sineTable.tick(index)+sineTable.tick(index2))*gain*0.5; } Bonus solution: Additive.cpp and Additive.h . Stereo Echo Reusing the result of the previous exercise, create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo. Solution: Basic solution: crazy_sine_stereo.zip Solution with dynamic memory allocation: crazy_sine_stereo_dyn.zip Solution: In MyDsp.h : Sine sine; Echo echo0, echo1; }; In MyDsp.cpp : MyDsp::MyDsp() : AudioStream(AUDIO_OUTPUTS, new audio_block_t*[AUDIO_OUTPUTS]), sine(AUDIO_SAMPLE_RATE_EXACT), echo0(AUDIO_SAMPLE_RATE_EXACT,10000), echo1(AUDIO_SAMPLE_RATE_EXACT,7000) { ... // setting up DSP objects echo0.setDel(10000); echo0.setFeedback(0.5); echo1.setDel(7000); echo1.setFeedback(0.4); ... // processing buffers for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { // DSP float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; ... }","title":" 3: Digital Audio Systems Architectures and Audio Callback "},{"location":"lectures/audio-sys/#digital-audio-systems-architectures-and-audio-callback","text":"By the end of this lecture, you should be able to produce sound with your Teensy and have a basic understanding of the software and hardware architecture of embedded audio systems.","title":"Digital Audio Systems Architectures and Audio Callback"},{"location":"lectures/audio-sys/#basic-architecture-of-a-digital-audio-system","text":"All digital audio systems have an architecture involving at least an ADC and/or a DAC. Audio samples are processed on a computer (i.e., CPU, microcontroller, DSP, etc.) typically in an audio callback and are transmitted to the DAC and/or received from the ADC: The format of audio samples depends on the hardware configuration of the system.","title":"Basic Architecture of a Digital Audio System"},{"location":"lectures/audio-sys/#architecture-of-embedded-audio-systems-such-as-the-teensy","text":"In embedded audio systems, the component implementing the audio ADC and DAC is called an \"Audio Codec.\" This name is slightly ambiguous because it is also used in the context of audio compression (e.g., mp3) to designate a totally different concept. In the case of the Teensy kits that are provided to you as part of this class, the audio codec we use is an SGTL5000. It is mounted on a shield/sister board that has the same form factor as the Teensy. Audio samples are sent and received between the Cortex M7 and the audio codec using the i2s protocol. As a microcontroller, the Cortex M7 has its own analog inputs which can be used to retrieve sensor datas (e.g., potentiometers, etc.). These analog inputs cannot be used for audio because of their limited precision and sampling rate. We'll briefly show in this lecture about control how these analog inputs can be used to use sensors to control audio algorithms running on the Teensy. Teensy and Audio Shield Overview","title":"Architecture of Embedded Audio Systems Such as the Teensy"},{"location":"lectures/audio-sys/#concept-of-audio-blocks-buffers-audio-rate-and-control-rate","text":"A large number of audio samples must be processed and transmitted every second. For example, if the sampling rate of the system is 48 kHz, 48000 samples will be processed in one second. Digital audio is extremely demanding and if one sample is missed, the result on the produced sound will be very audible. Most processors cannot process and transmit samples one by one which is why buffers need to be used. Hence, most digital audio systems will process audio as \"blocks.\" The smallest size of a block will be determined by the performance of the system. On a modern computer running an operating system such as Windows, MacOS or Linux, the standard block size is usually 256 samples. In that case, the audio callback will process and then transmit to the DAC 256 samples all at once. An audio callback function typically takes the following form: void audioCallback(float *inputs, float *outputs){ // control rate portion int gain = mainVolume; for(int i=0; i<blockSize; i++){ // audio rate portion outputs[i] = inputs[i]*gain; } } audioCallback is called every time a new buffer is needed by the audio interface (ADC/DAC). For example, if the sampling rate is 48 kHz and the block size is 256 samples, audioCallback will be called 187.5 (48000/256) per seconds. Here, the for loop parses the input buffer and copy it to the output by modifying its gain. Note that gain is set outside of the for loop. That's a very common thing to do in the field of real-time audio processing: what happens outside of the for loop is called the control rate and what happens inside the for loop is called the audio rate . The parameters of an audio program are typically processed at control rate since user interface elements usually run at a much lower rate than audio. Block size is directly linked to the audio latency of the system by the following formula: latency = BS/fs where latency is in seconds. Hence, the greater the block size, the larger the latency. For example, a block size of 256 samples at a sampling rate of 48 kHz will induce a latency of approximately 5ms. If the system has an audio input, this value has to be doubled, of course. A latency of 10ms might not seem like a lot but if the system is used for music purposes, this might be perceived by the performer. Embedded systems such as the Teensy can achieve much lower latencies than regular computers because of their lightness. Hence, the block size of your Teensy can be as small as 8!","title":"Concept of Audio Blocks (Buffers), Audio Rate, and Control Rate"},{"location":"lectures/audio-sys/#first-audio-program-on-the-teensy-crazy-sine","text":"The course repository hosts an example containing a program synthesizing a sine wave on the Teensy and controlling its frequency: crazy-sine . This program contains all the building blocks of a real-time audio program including... the audio callback which can be found in AudioDsp.cpp ! The audio callback is implemented in this class in the update method and take the following shape: #define MULT_16 32767 void MyDsp::update(void) { audio_block_t* outBlock[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; channel++) { outBlock[channel] = allocate(); if (outBlock[channel]) { for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { float currentSample = echo.tick(sine.tick())*0.5; currentSample = max(-1,min(1,currentSample)); int16_t val = currentSample*MULT_16; outBlock[channel]->data[i] = val; } transmit(outBlock[channel], channel); release(outBlock[channel]); } } } The update method is called every time a new audio buffer is needed by the system. A new audio buffer audioBlock containing AUDIO_OUTPUTS channels is first created. For every audio channel, memory is allocated and a full block of samples is computed. Individual samples resulting from computing a sine wave through an echo ( echo and sine are defined in the libraries folder and implement an echo and a sine wave oscillator, respectively) are stored in currentSample . currentSample is a floating point number whose range is {-1;1}. This is a standard in the world of digital audio, hence, a signal actually ranging between {-1;1} will correspond to the \"loudest\" sound that can be played on a given system. max(-1,min(1,currentSample)); ensures that currentSample doesn't exceed this range. AUDIO_BLOCK_SAMPLES corresponds to the block size (256 samples by default on the Teensy, but this value can potentially be adjusted). The values contained in currentSample (between -1 and 1) must be converted to 16 bits signed integers (to ensure compatibility with the rest of the Teensy audio library). For that, we just have to multiply currentSample by 2^{16-1} (if we were looking at unsigned integers, which can happen on some system, we would multiply currentSample by 2^{16} ). Note that currentSample is multiplied by 0.5 to control the output gain of the system here (we'll see later in this class that echos tend to add energy to the system hence we must limit the gain of the output signal to prevent potential saturation). Once a full block has been computed, it is transmitted to the rest of the system using the transmit function. Once this is done, the memory that was allocated for the audio block is freed using the release function. The update method is called over and over until the Teensy is powered out.","title":"First Audio Program on the Teensy: crazy-sine"},{"location":"lectures/audio-sys/#c-sine-wave-oscillator","text":"Sine wave are at the basis of many algorithms in the field of audio. The sound of a sine wave is what we call a \"pure tone\" since it only has a single harmonic. One of the consequences of this is that all sounds can be synthesized using a combination of sine waves ( Fourier transform ). From a mathematical standpoint, a sine oscillator can be implemented with the following differential equation: x(t) = Asin(\\omega t + \\phi) with: A : the peak amplitude \\omega = 2 \\pi f : the radian frequency (rad/sec) f : the frequency in Hz t : the time seconds \\phi : the initial phase (radians) x(t) could be translated to C++ by writing something like ( \\phi is ignored here): float currentSample = A*std::sin(2*PI*f*t); however sine oscillators are rarely implemented as such since calling the std::sin function at every sample can be quite computationally expensive. For that reason, it is better to pre-compute the sine wave and store it in a wave table before computation starts. That kind of algorithm is then called a \"wave table oscillator.\" Sine.cpp , which is used in crazy-sine is a good example of that. It uses SineTable.cpp which pre-computes a sine table: table = new float[size]; for(int i=0; i<size; i++){ table[i] = std::sin(i*2.0*PI/size); } and then makes it accessible through the tick (compute) method: float SineTable::tick(int index){ return table[index%tableSize]; } The size of the table plays an important role on the quality of the generated sound. The greater the size, the more accurate/pure the sine wave. A low resolution sine wave will produce more distortion. In Sine.cpp , the sine wave table has a size of 2^{14} which presents a good compromise between sound quality and memory. It is important to keep in mind that when working with embedded systems memory is also an important factor to take into account. The sine table is then read with a \"phasor.\" A phasor produces a ramp signal which is reset at a certain frequency. It can also be seen as a sawtooth wave. Phasor.cpp is used for that purpose and its tick method is defined as: float Phasor::tick(){ float currentSample = phasor; phasor += phasorDelta; phasor = phasor - std::floor(phasor); return currentSample; } It hence ramps from 0 to 1 at a given frequency. The phasor object in Sine.cpp is used to read the sine table by adjusting the range of its output: float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(index)*gain; }","title":"C++ Sine Wave Oscillator"},{"location":"lectures/audio-sys/#exercises","text":"","title":"Exercises"},{"location":"lectures/audio-sys/#looping-through-a-small-tune","text":"In the world of music technology, musical notes are usually represented by MIDI numbers . In MIDI, each pitch of the chromatic scale has a number between 0 and 127 associated to it: https://djip.co/blog/logic-studio-9-midi-note-numbers As you can see, middle C (Do) corresponds to number 72. MIDI note numbers can be converted to a frequency using the following formula: f=2^{(d-69)/12}.440 where d is the MIDI number. Write a small tune/song looping through at least 5 notes and play it with the crazy-sine program on your Teensy. Hint: For that, you'll probably have to replace the myDsp.setFreq(random(50,1000)); line of of crazy-sine.ino by something else. Solution: In crazy-sine.ino : #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); float mtof(float note){ return pow(2.0,(note-69.0)/12.0)*440.0; } int tune[] = {62,78,65,67,69}; int cnt = 0; void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { myDsp.setFreq(mtof(tune[cnt])); cnt = (cnt+1)%5; delay(500); }","title":"Looping Through a Small Tune"},{"location":"lectures/audio-sys/#basic-additive-synthesis","text":"One of the most basic kind of sound synthesis is \"additive synthesis.\" In consists of adding multiple sine wave oscillators together to \"sculpt\" the timbre of a sound. Both the frequency and the gain of each individual oscillator can then be used to change the properties of the synthesized sound. A simple additive synthesizer could be implemented from the crazy-sine example by declaring multiple instances of sine . E.g.: float currentSample = echo.tick(sine0.tick()*gain0 + sine1.tick()*gain1); but the problem with that option is that memory will be allocated twice for the sineTable array which is a terrible idea in the context of our embedded audio system with very little memory. Instead, the additive synthesizer should reuse the same instance of sineTable for each oscillator. In the tick method of Sine.cpp , try to call the sineTable a second time after float currentSample = sineTable[index]*gain; to add a second oscillator to the generated sample. The value of its index could be something like index = (int) (index*1.5)%SINE_TABLE_SIZE; so that the frequency of the second oscillator is one fifth above the main frequency. In other words, the differential equation of the synth should be: x(t) = sin(2 \\pi f t) + sin(2 \\pi (1.5f) t) Hint: Beware of clipping! Adding two sine waves together even though they don't have the same frequency will likely produce a signal whose range exceeds {-1;1}: you should take that into account for your final product. Solution: In Sine.cpp : float Sine::tick(){ int index = phasor.tick()*SINE_TABLE_SIZE; int index2 = (int) (index*1.5)%SINE_TABLE_SIZE; return (sineTable.tick(index)+sineTable.tick(index2))*gain*0.5; } Bonus solution: Additive.cpp and Additive.h .","title":"Basic Additive Synthesis"},{"location":"lectures/audio-sys/#stereo-echo","text":"Reusing the result of the previous exercise, create a second instance of echo (connected to the same instance of sine ) with different parameters from the first one that will be connected to the second channel of the output (i.e., the first instance should be connected to the left channel and the second one to the right channel). The final algorithm should look like this: float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; Hint: Beware of memory allocation again! Make sure that the maxim delay of your echo (on the 2 parameters of the class constructor) doesn't exceed 10000 for now for both instances of the echo. Solution: Basic solution: crazy_sine_stereo.zip Solution with dynamic memory allocation: crazy_sine_stereo_dyn.zip Solution: In MyDsp.h : Sine sine; Echo echo0, echo1; }; In MyDsp.cpp : MyDsp::MyDsp() : AudioStream(AUDIO_OUTPUTS, new audio_block_t*[AUDIO_OUTPUTS]), sine(AUDIO_SAMPLE_RATE_EXACT), echo0(AUDIO_SAMPLE_RATE_EXACT,10000), echo1(AUDIO_SAMPLE_RATE_EXACT,7000) { ... // setting up DSP objects echo0.setDel(10000); echo0.setFeedback(0.5); echo1.setDel(7000); echo1.setFeedback(0.4); ... // processing buffers for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { // DSP float sineSample = sine.tick(); float currentSampleL = echo0.tick(sineSample)*0.5; float currentSampleR = echo1.tick(sineSample)*0.5; ... }","title":"Stereo Echo"},{"location":"lectures/control/","text":"Hardware Control and Audio Codec Configuration The two main goals of this lecture are: to show you how to control DSP algorithms running on your Teensy using hardware controllers (i.e., potentiometers and buttons); to give you a basic understanding of how audio codecs work and how they can be configured using the i2c protocol. Hardware Control Electronic Basics While the goal of this class is not to teach electronics nor to make projects involving complicated circuitry, some basic circuits do need to be implemented in order to control the various parameters of the DSP algorithms studied in class using hardware controllers such as buttons and potentiometers. Hence, in case you feel like your electronics skills are a bit rusty, feel free to review the following page: https://ccrma.stanford.edu/wiki/Introduction_to_Electronics_(condensed) . Teensy 4.0 Pinout The Teensy pins map can be seen in the following figure (directly taken from the PJRC website ). Most pins can be used as digital I/Os. Some pins noted \"A(N)\" can be used as analog inputs. Please, also note that 3.3v power can be retrieved from the top right corner pin and the ground from the top left pin. Make sure to never connect the 5.5v Vin pin to any other pin of the Teensy: that would probably fry it (the Cortex M7 inside the Teensy operates at 3.3v)! Teensy pinout. The Teensy audio shield uses a bunch of pins on the Teensy for i2c and i2s communication: Teensy audio shield pins. This means that these pins (besides GND and 3.3v, of course) cannot be used for something else (i.e., connecting external sensors). Bringing Power to Your Breadboard The first step in making your first circuit with the Teensy is to bring power to the breadboard included in your kit using jumper wires: Teensy connected to the breadboard. Basically connect the 3.3v pin to the red strip and the GND pin to the black strip of the breadboard. WARNING: Do not connect the 5.5v pin to the breadboard! Adding a Rotary Potentiometer to the Circuit Your kit should come with a couple of rotary potentiometers: Rotary potentiometer mounted on the breadboard. Place it on the breadboard, and connect its leftmost pin to power and its rightmost pin to the ground. Finally, connect its center pin to the A0 pin of the Teensy using a jumper wire. Please, note that we're using this pin since it is not used by the audio shield (see previous section). Testing the Potentiometer In the current configuration, the potentiometer will deliver a 3.3v current at its center pin if it is fully turned to the right side, and 0v if it is fully turned to the left side. The following Teensy program: void setup() { Serial.begin(9600); } void loop() { int sensorValue = analogRead(A0); Serial.println(sensorValue); delay(100); } displays the values measured at the A0 pin on the Teensy in the serial debugger. Values should be between 0 and 1023 (10 bits values). Make sure that the values you're getting are consistent with the position of the potentiometer. Controlling DSP Parameters With the Potentiometer Now that you know how to retrieve potentiometer values in the Teensy, plugging it to your audio DSP should be pretty straightforward. Hence, we can reuse the crazy-sine example and do: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } Note that sensorValue needs to be turned into a frequency in hertz so we just add 100 to it to get a frequency between 100 and 1123 hertz. Now take some time to have fun ;)! Using a Button Using a button with the Teensy is slightly more involving since the use of a pulldown resistor is required (alternatively, a pullup resistor could be used, of course). This is due to the fact that buttons are \"just\" circuit breakers: they don't have a dedicated output pin like potentiometers. The pulldown resistor is used to suck potential floating currents out of the output pin of the button in order to get a stable signal to be measured on the Teensy. Hence, the following circuit must be implemented: Circuit to connect a button to the Teensy. There's no need to use an analog pin on the Teensy to measure the voltage at the output of the button since we're looking at discrete values here (0 or 1). Hence, the button shall be connected to a digital pin (number 0, for example). Expanding on the previous example, we could write: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { pinMode(0, INPUT); // configuring digital pin 0 as an input AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { if (digitalRead(0)) { // button is pressed myDsp.setGain(1); } else { myDsp.setGain(0); } int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } (Assuming that a setGain method has been implemented, which is not the case in the previous example. It shouldn't be too hard though ;) ) Exercise: Looping Between Notes by Pressing a Button Expand the \"note looper\" that you implemented as part of the audio system course so that new notes are triggered when a button is pressed (as opposed to be triggered automatically). Every time the button is pressed, a new note is produced. This means that you'll have to turn your push button into a switch using software techniques... Finally, make sure that gain is controllable using a rotary potentiometer. Solution: In crazy-sine.ino : #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); float mtof(float note){ return pow(2.0,(note-69.0)/12.0)*440.0; } int tune[] = {62,78,65,67,69}; int cnt = 0; bool change = true; bool on = false; void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); pinMode(0, INPUT); } void loop() { if (digitalRead(0)) { // button is pressed if(!on) change = true; on = true; } else { if(on) change = true; on = false; } if(change){ // if the status of the button changed if(on){ // if the button is pressed myDsp.setFreq(mtof(tune[cnt])); cnt = (cnt+1)%5; } change = false; // status changed } } Audio Codec Configuration Audio Codec An audio codec is a hardware component providing an ADC and a DAC for audio purposes. Hence, it typically has analog audio inputs and outputs (stereo, in general) and digital audio inputs and outputs. Most audio codecs support standard audio sampling rate (i.e., 44.1, 48kHz, etc.) and bit depth (i.e., 16, 24 bits, etc.). Some high-end audio codecs also support higher sampling rates (e.g., 96, 192 kHz, etc.) and bit depth (32 bits, mostly) as well as more than a stereo interface (e.g., 4x4, 8x8, etc.). The price range of audio codecs can vary significantly impacting the quality of the components, i.e., audio is usually extremely sensitive to the quality of the hardware components of a system. Audio codecs usually use two different communication channels to connect to the processor unit (whether it's a CPU, a microcontroller, etc.): an i2c bus is used to configure the codec, an i2s bus is used to transmit digital audio data. i2c (pronounced I-squared-C) is a serial communication protocol heavily used in the field of microelectronics. Most digital elements in an electronic circuit communicate using i2c. i2s (pronounced I-squared-S) is also a serial communication protocol but targeting specifically audio applications. We'll see that they're very close to each other in practice later in this class. The Teensy Audio Shield hosts an audio codec (SGTL5000) which is connected to the Teensy using the following model: Interfacing of a microcontroller with an audio codec. You can check the Teensy audio shield connection map in the Teensy Pinout section for more details on how the audio shield is actually connected to the Teensy. Before audio data can be streamed to the audio codec through i2s, it needs to be configured with an audio driver which basically just sends a set of instructions from the microcontroller to the codec using i2c. The goal of this lecture is to get a basic understanding of how audio drivers work in the context of embedded systems. Quick Tour of the SGTL5000 The SGTL5000 is a low-power 24-bit, 8 kHz to 96 kHz audio codec. Its data sheet can be found on the course repository (feel free to download it now because you'll extensively need it later in this lecture). On the first page of the data sheet, you will find a block diagram indicating the different components of the codec: Block diagram of the SGTL5000 As you can see, the SGTL5000 hosts a stereo ADC and DAC. The codec has a stereo line input and a mono mic input (both accessible through solderable pins on the audio shield). The difference between the two is that the line input goes straight to the ADC while the mic input goes through a preamp first. The gain of the preamp can be adjusted independently from that of the line input. A similar pattern is used for the outputs and the DAC which are available as line outputs (through solderable pins on the audio shield) or amplified outputs (through the headphone jack on the audio shield). Finally, the codec also has a digital interface which is used for i2c (depicted at the bottom of the block diagram) and i2s (depicted on the left side of the block diagram) communication. Configuring an Audio Codec All audio codecs work the same way and are configured through their i2c bus. A system of register/value is used for that. A register corresponds to a set of parameters and a 16 bits value can be provided to configure them. A list of all available registers of the SGTL5000 can be seen on page 31-59 of the data sheet . Have a quick look at it! For example, register 0x0010 (which is documented on p. 36) allows us to configure the DAC left and right channel volume in dB. Hence setting that register to the 3C3C (0011110000111100 binary value) will set the volume to 0dB on both channels. If you're hex/binary is a bit rusty, you can use this tool: https://www.rapidtables.com/convert/number/hex-to-binary.html to carry out the conversion. Audio Codec Driver Writing an audio codec driver consists of sending the right sequence of register/value through i2c to the codec. This will set the signal routing, the i2s format, the sampling rate, the bit depth, etc. control_sgtl5000.cpp of the Teensy Audio Library implements a driver for the SGTL5000 codec. The write method can be used to set a register and its corresponding 16-bit value. Note that the Arduino Wire library is used for that. The enable method sets a bunch of register values to provide a basic working configuration to the codec in the context of the Teensy. A bunch of methods are implemented to set high-level parameters of the codec, such as volume , micGain , lineInLevel , etc. Note that a bunch of macros are defined at the beginning for various registers (which constitutes a potential alternative to the codec datasheet, etc.). In most cases, calling: AudioControlSGTL5000 audioShield; audioShield.enable(); in the .ino file will be sufficient to get the audio codec going and send it i2s data. Since the write method of AudioControlSGTL5000 is protected, it cannot be called outside of the class. Hence, to write custom methods to configure the codec and which are not currently available in AudioControlSGTL5000 , one would have to write a custom version of control_sgtl5000.cpp .","title":" 4: Hardware Control and Audio Codec Configuration "},{"location":"lectures/control/#hardware-control-and-audio-codec-configuration","text":"The two main goals of this lecture are: to show you how to control DSP algorithms running on your Teensy using hardware controllers (i.e., potentiometers and buttons); to give you a basic understanding of how audio codecs work and how they can be configured using the i2c protocol.","title":"Hardware Control and Audio Codec Configuration"},{"location":"lectures/control/#hardware-control","text":"","title":"Hardware Control"},{"location":"lectures/control/#electronic-basics","text":"While the goal of this class is not to teach electronics nor to make projects involving complicated circuitry, some basic circuits do need to be implemented in order to control the various parameters of the DSP algorithms studied in class using hardware controllers such as buttons and potentiometers. Hence, in case you feel like your electronics skills are a bit rusty, feel free to review the following page: https://ccrma.stanford.edu/wiki/Introduction_to_Electronics_(condensed) .","title":"Electronic Basics"},{"location":"lectures/control/#teensy-40-pinout","text":"The Teensy pins map can be seen in the following figure (directly taken from the PJRC website ). Most pins can be used as digital I/Os. Some pins noted \"A(N)\" can be used as analog inputs. Please, also note that 3.3v power can be retrieved from the top right corner pin and the ground from the top left pin. Make sure to never connect the 5.5v Vin pin to any other pin of the Teensy: that would probably fry it (the Cortex M7 inside the Teensy operates at 3.3v)! Teensy pinout. The Teensy audio shield uses a bunch of pins on the Teensy for i2c and i2s communication: Teensy audio shield pins. This means that these pins (besides GND and 3.3v, of course) cannot be used for something else (i.e., connecting external sensors).","title":"Teensy 4.0 Pinout"},{"location":"lectures/control/#bringing-power-to-your-breadboard","text":"The first step in making your first circuit with the Teensy is to bring power to the breadboard included in your kit using jumper wires: Teensy connected to the breadboard. Basically connect the 3.3v pin to the red strip and the GND pin to the black strip of the breadboard. WARNING: Do not connect the 5.5v pin to the breadboard!","title":"Bringing Power to Your Breadboard"},{"location":"lectures/control/#adding-a-rotary-potentiometer-to-the-circuit","text":"Your kit should come with a couple of rotary potentiometers: Rotary potentiometer mounted on the breadboard. Place it on the breadboard, and connect its leftmost pin to power and its rightmost pin to the ground. Finally, connect its center pin to the A0 pin of the Teensy using a jumper wire. Please, note that we're using this pin since it is not used by the audio shield (see previous section).","title":"Adding a Rotary Potentiometer to the Circuit"},{"location":"lectures/control/#testing-the-potentiometer","text":"In the current configuration, the potentiometer will deliver a 3.3v current at its center pin if it is fully turned to the right side, and 0v if it is fully turned to the left side. The following Teensy program: void setup() { Serial.begin(9600); } void loop() { int sensorValue = analogRead(A0); Serial.println(sensorValue); delay(100); } displays the values measured at the A0 pin on the Teensy in the serial debugger. Values should be between 0 and 1023 (10 bits values). Make sure that the values you're getting are consistent with the position of the potentiometer.","title":"Testing the Potentiometer"},{"location":"lectures/control/#controlling-dsp-parameters-with-the-potentiometer","text":"Now that you know how to retrieve potentiometer values in the Teensy, plugging it to your audio DSP should be pretty straightforward. Hence, we can reuse the crazy-sine example and do: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } Note that sensorValue needs to be turned into a frequency in hertz so we just add 100 to it to get a frequency between 100 and 1123 hertz. Now take some time to have fun ;)!","title":"Controlling DSP Parameters With the Potentiometer"},{"location":"lectures/control/#using-a-button","text":"Using a button with the Teensy is slightly more involving since the use of a pulldown resistor is required (alternatively, a pullup resistor could be used, of course). This is due to the fact that buttons are \"just\" circuit breakers: they don't have a dedicated output pin like potentiometers. The pulldown resistor is used to suck potential floating currents out of the output pin of the button in order to get a stable signal to be measured on the Teensy. Hence, the following circuit must be implemented: Circuit to connect a button to the Teensy. There's no need to use an analog pin on the Teensy to measure the voltage at the output of the button since we're looking at discrete values here (0 or 1). Hence, the button shall be connected to a digital pin (number 0, for example). Expanding on the previous example, we could write: #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); void setup() { pinMode(0, INPUT); // configuring digital pin 0 as an input AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); } void loop() { if (digitalRead(0)) { // button is pressed myDsp.setGain(1); } else { myDsp.setGain(0); } int sensorValue = analogRead(A0); float freq = sensorValue + 100; myDsp.setFreq(freq); } (Assuming that a setGain method has been implemented, which is not the case in the previous example. It shouldn't be too hard though ;) )","title":"Using a Button"},{"location":"lectures/control/#exercise-looping-between-notes-by-pressing-a-button","text":"Expand the \"note looper\" that you implemented as part of the audio system course so that new notes are triggered when a button is pressed (as opposed to be triggered automatically). Every time the button is pressed, a new note is produced. This means that you'll have to turn your push button into a switch using software techniques... Finally, make sure that gain is controllable using a rotary potentiometer. Solution: In crazy-sine.ino : #include <Audio.h> #include \"MyDsp.h\" MyDsp myDsp; AudioOutputI2S out; AudioControlSGTL5000 audioShield; AudioConnection patchCord0(myDsp,0,out,0); AudioConnection patchCord1(myDsp,0,out,1); float mtof(float note){ return pow(2.0,(note-69.0)/12.0)*440.0; } int tune[] = {62,78,65,67,69}; int cnt = 0; bool change = true; bool on = false; void setup() { AudioMemory(2); audioShield.enable(); audioShield.volume(0.5); pinMode(0, INPUT); } void loop() { if (digitalRead(0)) { // button is pressed if(!on) change = true; on = true; } else { if(on) change = true; on = false; } if(change){ // if the status of the button changed if(on){ // if the button is pressed myDsp.setFreq(mtof(tune[cnt])); cnt = (cnt+1)%5; } change = false; // status changed } }","title":"Exercise: Looping Between Notes by Pressing a Button"},{"location":"lectures/control/#audio-codec-configuration","text":"","title":"Audio Codec Configuration"},{"location":"lectures/control/#audio-codec","text":"An audio codec is a hardware component providing an ADC and a DAC for audio purposes. Hence, it typically has analog audio inputs and outputs (stereo, in general) and digital audio inputs and outputs. Most audio codecs support standard audio sampling rate (i.e., 44.1, 48kHz, etc.) and bit depth (i.e., 16, 24 bits, etc.). Some high-end audio codecs also support higher sampling rates (e.g., 96, 192 kHz, etc.) and bit depth (32 bits, mostly) as well as more than a stereo interface (e.g., 4x4, 8x8, etc.). The price range of audio codecs can vary significantly impacting the quality of the components, i.e., audio is usually extremely sensitive to the quality of the hardware components of a system. Audio codecs usually use two different communication channels to connect to the processor unit (whether it's a CPU, a microcontroller, etc.): an i2c bus is used to configure the codec, an i2s bus is used to transmit digital audio data. i2c (pronounced I-squared-C) is a serial communication protocol heavily used in the field of microelectronics. Most digital elements in an electronic circuit communicate using i2c. i2s (pronounced I-squared-S) is also a serial communication protocol but targeting specifically audio applications. We'll see that they're very close to each other in practice later in this class. The Teensy Audio Shield hosts an audio codec (SGTL5000) which is connected to the Teensy using the following model: Interfacing of a microcontroller with an audio codec. You can check the Teensy audio shield connection map in the Teensy Pinout section for more details on how the audio shield is actually connected to the Teensy. Before audio data can be streamed to the audio codec through i2s, it needs to be configured with an audio driver which basically just sends a set of instructions from the microcontroller to the codec using i2c. The goal of this lecture is to get a basic understanding of how audio drivers work in the context of embedded systems.","title":"Audio Codec"},{"location":"lectures/control/#quick-tour-of-the-sgtl5000","text":"The SGTL5000 is a low-power 24-bit, 8 kHz to 96 kHz audio codec. Its data sheet can be found on the course repository (feel free to download it now because you'll extensively need it later in this lecture). On the first page of the data sheet, you will find a block diagram indicating the different components of the codec: Block diagram of the SGTL5000 As you can see, the SGTL5000 hosts a stereo ADC and DAC. The codec has a stereo line input and a mono mic input (both accessible through solderable pins on the audio shield). The difference between the two is that the line input goes straight to the ADC while the mic input goes through a preamp first. The gain of the preamp can be adjusted independently from that of the line input. A similar pattern is used for the outputs and the DAC which are available as line outputs (through solderable pins on the audio shield) or amplified outputs (through the headphone jack on the audio shield). Finally, the codec also has a digital interface which is used for i2c (depicted at the bottom of the block diagram) and i2s (depicted on the left side of the block diagram) communication.","title":"Quick Tour of the SGTL5000"},{"location":"lectures/control/#configuring-an-audio-codec","text":"All audio codecs work the same way and are configured through their i2c bus. A system of register/value is used for that. A register corresponds to a set of parameters and a 16 bits value can be provided to configure them. A list of all available registers of the SGTL5000 can be seen on page 31-59 of the data sheet . Have a quick look at it! For example, register 0x0010 (which is documented on p. 36) allows us to configure the DAC left and right channel volume in dB. Hence setting that register to the 3C3C (0011110000111100 binary value) will set the volume to 0dB on both channels. If you're hex/binary is a bit rusty, you can use this tool: https://www.rapidtables.com/convert/number/hex-to-binary.html to carry out the conversion.","title":"Configuring an Audio Codec"},{"location":"lectures/control/#audio-codec-driver","text":"Writing an audio codec driver consists of sending the right sequence of register/value through i2c to the codec. This will set the signal routing, the i2s format, the sampling rate, the bit depth, etc. control_sgtl5000.cpp of the Teensy Audio Library implements a driver for the SGTL5000 codec. The write method can be used to set a register and its corresponding 16-bit value. Note that the Arduino Wire library is used for that. The enable method sets a bunch of register values to provide a basic working configuration to the codec in the context of the Teensy. A bunch of methods are implemented to set high-level parameters of the codec, such as volume , micGain , lineInLevel , etc. Note that a bunch of macros are defined at the beginning for various registers (which constitutes a potential alternative to the codec datasheet, etc.). In most cases, calling: AudioControlSGTL5000 audioShield; audioShield.enable(); in the .ino file will be sufficient to get the audio codec going and send it i2s data. Since the write method of AudioControlSGTL5000 is protected, it cannot be called outside of the class. Hence, to write custom methods to configure the codec and which are not currently available in AudioControlSGTL5000 , one would have to write a custom version of control_sgtl5000.cpp .","title":"Audio Codec Driver"},{"location":"lectures/dsp1/","text":"Audio Processing Basics I This lecture and the following one present a selection of audio processing and synthesis algorithms. It is in no way comprehensive: the goal is just to give you a sense of what's out there. All these algorithms have been extensively used during the second half of the twentieth century by musicians and artists, especially within the computer music community. White Noise White noise is a specific kind of signal in which there's an infinite number of harmonics all having the same level. In other words, the spectrum of white noise looks completely flat. White noise is produced by generating random numbers between -1 and 1. Noise.cpp demonstrates how this can be done in C++ using the rand() function: Noise::Noise() : randDiv(1.0/RAND_MAX){} float Noise::tick(){ return rand()*randDiv*2 - 1; } The Simple Filter: One Zero section presents a use example of white noise. Wave Shape Synthesis Wave Shape synthesis is one of the most basic sound synthesis technique. It consists of using oscillators producing waveforms of different shapes to generate sound. The most standard wave shapes are: sine wave , square wave , triangle wave , sawtooth wave . The crazy-sine example can be considered as \"wave shape synthesis\" in that regard. The crazy-saw example is very similar to crazy-sine , but it's based on a sawtooth wave instead. The sawtooth wave is created by using a phasor object. Just as a reminder, a phasor produces a signals tamping from 0 to 1 at a given frequency, it can therefore be seen as a sawtooth wave. Since the range of oscillators must be bounded between -1 and 1, we adjusts the output of the phasor such that: float currentSample = sawtooth.tick()*2 - 1; Feel free to try the crazy-saw example at this point. Amplitude Modulation (AM) Synthesis Amplitude modulation synthesis consists of modulating the amplitude of a signal with another one. Sine waves are typically used for that: Amplitude Modulation (Source: Wikipedia ) When the frequency of the modulator is low (bellow 20Hz), our ear is able to distinguish each independent \"beat,\" creating a tremolo effect. However, above 20Hz two side bands (if sine waves are used) start appearing following this rule: Amplitude Modulation Spectrum (Source: Wikipedia ) The mathematical proof of this can be found on Julius Smith's website . Am.cpp implements a sinusoidal amplitude modulation synthesizer: float Am::tick(){ int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float posMod = sineTable.tick(mIndex)*0.5 + 0.5; return sineTable.tick(cIndex)*(1 - posMod*modIndex)*gain; } Note that phasors are used instead of \"complete\" sine wave oscillators to save the memory of an extra sine wave table. The range of the modulating oscillator is adjusted to be {0,1} instead of {-1,1}. The amplitude parameter of the modulating oscillator is called the index of modulation and its frequency, the frequency of modulation . In practice, the same result could be achieved using additive synthesis and three sine wave oscillators but AM allows us to save one oscillator. Also, AM is usually used an audio effect and modulation is applied to an input signal in that case instead of a sine wave. Sidebands will then be produced for each harmonic of the processed sound. The am example demonstrates a use case of an AM synthesizer. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. Frequency Modulation (FM) Synthesis Frequency modulation synthesis consists of modulating the frequency of an oscillator with another one: Frequency Modulation (Source: Wikipedia ) which mathematically can be expressed as: x(t) = A_c\\sin[\\omega_ct + \\phi_c + A_m\\sin(\\omega_mt + \\phi_m)] where c denotes the carrier and m , the modulator. As for AM, the frequency of the modulating oscillator is called the frequency of modulation and the amplitude of the modulating oscillator, the index of modulation . Unlike AM, the value of the index of modulation can exceed 1 which will increase the number of sidebands. FM is not limited to two sidebands and can have an infinite number of sidebands depending on the value of the index. The mathematical rational behind this can be found on Julius Smith's website . fm.cpp provides a simple example of how an FM synthesizer can be implemented: float Fm::tick(){ int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float modulator = sineTable.tick(mIndex); cPhasor.setFrequency(cFreq + modulator*modIndex); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } Note that as for the AM example, we're saving an extra sine wave table by using the same one for both oscillators. The examples folder of the course repository hosts a simple Teensy program illustrating the use of FM. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. FM synthesis was discovered in the late 1960s by John Chowning at Stanford University in California. He's now considered as one of the funding fathers of music technology and computer music. FM completely revolutionized the world of music in the 1980s by allowing Yamaha to produce the first commercial digital synthesizers: the DX7 which met a huge success. FM synthesis is the second most profitable patent that Stanford ever had. Simple Filter: One Zero Filters are heavily used in the field of audio processing. In fact, designing filters is a whole field by itself. They are at the basis of many audio effects such as Wah guitar pedals, etc. From an algorithmic standpoint, the most basic filter is what we call a \"one zero\" filter which means that its transfer function only has numerators and no denominators. The differential equation of a one zero filter can be expressed as: y(n) = b_0x(n) + b_1x(n-1) where b_1 is \"the zero\" of the filter (also called feed forward coefficient ), b_0 can be discarded as it is equal to 1 in most cases. One zero filters can either be used as a lowpass if the value of b_1 is positive or as a highpass if b_1 is negative. The frequency response of the filter which is be obtained with H(e^{j \\omega T}) = b_0 + b_1e^{-j \\omega T} can be visualized on Julius Smith's website . Note that the gain of the signal is amplified on the second half of the spectrum which needs to be taken into account if this filter is used to process audio (once again, the output signal must be bounded within {-1,1}). OneZero.cpp implements a one zero filter: float OneZero::tick(float input){ float output = input + del*b1; del = input; return output*0.5; } Note that we multiply the output by 0.5 to normalize the output gain. The filtered-noise example program for the Teensy demonstrates the use of OneZero.cpp by feeding white noise in it. The value of b_1 can be changed by pressing the \"Mode\" button on the board, give it a try! Exercises LFO: Low Frequency Oscillator An LFO is an oscillator whose frequency is below the human hearing range (20 Hz). LFOs are typically used to create vibrato. In that case, the frequency of the LFO is usually set to 6 Hz. Modify the crazy-saw example so that notes are played slower (1 per second) and that some vibrato is added to the generated sound. Solution: In MyDsp.h : #include \"Sine.h\" ... private: float freq; Phasor sawtooth; Echo echo; Sine LFO; }; In MyDsp.cpp : MyDsp::MyDsp() : ... freq(440), sawtooth(AUDIO_SAMPLE_RATE_EXACT), echo(AUDIO_SAMPLE_RATE_EXACT,10000), LFO(AUDIO_SAMPLE_RATE_EXACT) { ... // setting up DSP objects echo.setDel(10000); echo.setFeedback(0.5); LFO.setFrequency(6); } ... // set sine wave frequency void MyDsp::setFreq(float f){ freq = f; } ... for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { // DSP sawtooth.setFrequency(freq*(1 + LFO.tick()*0.1)); float currentSample = echo.tick(sawtooth.tick()*2 - 1)*0.5; Towards the DX7 The DX7 carried out frequency modulation over a total of six oscillators that could be patched in different ways . So FM is not limited to two oscillators... Try to implement an FM synthesizer involving 3 oscillators instead of one. They should be connected in series: 3 -> 2 -> 1. Solution: (non-exhaustive) In Fm.cpp : float Fm::tick(){ int m0Index = m0Phasor.tick()*SINE_TABLE_SIZE; float modulator0 = sineTable.tick(m0Index); modulator1.setFrequency(m1Freq + modulator0*mod0Index); int m1Index = m1Phasor.tick()*SINE_TABLE_SIZE; float modulator1 = sineTable.tick(m1Index); cPhasor.setFrequency(cFreq + modulator1*mod1Index); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } And more exhaustive solution is provided in fm3 here . Smoothing In most cases, DSP parameters are executed at control rate. Moreover, the resolution of the value used to configure parameters is much lower than that of audio samples since it might come from a Graphical User Interface (GUI), a low resolution sensor ADC (e.g., arduino), etc. For all these reasons, changing the value of a DSP parameter will often result in a \"click\"/discontinuity. A common way to prevent this from happening is to interpolate between the values of the parameter using a \"leaky integrator.\" In signal processing, this can be easily implemented using a normalized one pole lowpass filter: y(n) = (1-s)x(n) + sy(n-1) where s is the value of the pole and is typically set to 0.999 for optimal results. Modify the crazy-saw example by \"smoothing\" the value of the frequency parameter by implementing the filter above with s=0.999 . Then slow down the rate at which frequency is being changed so that only two new values are generated per second. The result should sound quite funny :). Solution: In addition to Smooth.cpp and Smooth.h , in Phasor.h : int samplingRate; Smooth smooth; }; and Phasor.cpp : float Phasor::tick(){ float currentSample = phasor; phasor += smooth.tick(phasorDelta); phasor = phasor - std::floor(phasor); return currentSample; } Smoothing Potentiometer Values Try to use the smoothing function that you implemented in the previous step to smooth sensor values coming from a potential potentiometer controlling some parameter of one of the Teensy examples. The main idea is to get rid of sound artifacts when making abrupt changes in potentiometers.","title":" 6: Audio Processing Basics I "},{"location":"lectures/dsp1/#audio-processing-basics-i","text":"This lecture and the following one present a selection of audio processing and synthesis algorithms. It is in no way comprehensive: the goal is just to give you a sense of what's out there. All these algorithms have been extensively used during the second half of the twentieth century by musicians and artists, especially within the computer music community.","title":"Audio Processing Basics I"},{"location":"lectures/dsp1/#white-noise","text":"White noise is a specific kind of signal in which there's an infinite number of harmonics all having the same level. In other words, the spectrum of white noise looks completely flat. White noise is produced by generating random numbers between -1 and 1. Noise.cpp demonstrates how this can be done in C++ using the rand() function: Noise::Noise() : randDiv(1.0/RAND_MAX){} float Noise::tick(){ return rand()*randDiv*2 - 1; } The Simple Filter: One Zero section presents a use example of white noise.","title":"White Noise"},{"location":"lectures/dsp1/#wave-shape-synthesis","text":"Wave Shape synthesis is one of the most basic sound synthesis technique. It consists of using oscillators producing waveforms of different shapes to generate sound. The most standard wave shapes are: sine wave , square wave , triangle wave , sawtooth wave . The crazy-sine example can be considered as \"wave shape synthesis\" in that regard. The crazy-saw example is very similar to crazy-sine , but it's based on a sawtooth wave instead. The sawtooth wave is created by using a phasor object. Just as a reminder, a phasor produces a signals tamping from 0 to 1 at a given frequency, it can therefore be seen as a sawtooth wave. Since the range of oscillators must be bounded between -1 and 1, we adjusts the output of the phasor such that: float currentSample = sawtooth.tick()*2 - 1; Feel free to try the crazy-saw example at this point.","title":"Wave Shape Synthesis"},{"location":"lectures/dsp1/#amplitude-modulation-am-synthesis","text":"Amplitude modulation synthesis consists of modulating the amplitude of a signal with another one. Sine waves are typically used for that: Amplitude Modulation (Source: Wikipedia ) When the frequency of the modulator is low (bellow 20Hz), our ear is able to distinguish each independent \"beat,\" creating a tremolo effect. However, above 20Hz two side bands (if sine waves are used) start appearing following this rule: Amplitude Modulation Spectrum (Source: Wikipedia ) The mathematical proof of this can be found on Julius Smith's website . Am.cpp implements a sinusoidal amplitude modulation synthesizer: float Am::tick(){ int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float posMod = sineTable.tick(mIndex)*0.5 + 0.5; return sineTable.tick(cIndex)*(1 - posMod*modIndex)*gain; } Note that phasors are used instead of \"complete\" sine wave oscillators to save the memory of an extra sine wave table. The range of the modulating oscillator is adjusted to be {0,1} instead of {-1,1}. The amplitude parameter of the modulating oscillator is called the index of modulation and its frequency, the frequency of modulation . In practice, the same result could be achieved using additive synthesis and three sine wave oscillators but AM allows us to save one oscillator. Also, AM is usually used an audio effect and modulation is applied to an input signal in that case instead of a sine wave. Sidebands will then be produced for each harmonic of the processed sound. The am example demonstrates a use case of an AM synthesizer. Use the Rec and Mode button to cycle through the parameters of the synth and change their value.","title":"Amplitude Modulation (AM) Synthesis"},{"location":"lectures/dsp1/#frequency-modulation-fm-synthesis","text":"Frequency modulation synthesis consists of modulating the frequency of an oscillator with another one: Frequency Modulation (Source: Wikipedia ) which mathematically can be expressed as: x(t) = A_c\\sin[\\omega_ct + \\phi_c + A_m\\sin(\\omega_mt + \\phi_m)] where c denotes the carrier and m , the modulator. As for AM, the frequency of the modulating oscillator is called the frequency of modulation and the amplitude of the modulating oscillator, the index of modulation . Unlike AM, the value of the index of modulation can exceed 1 which will increase the number of sidebands. FM is not limited to two sidebands and can have an infinite number of sidebands depending on the value of the index. The mathematical rational behind this can be found on Julius Smith's website . fm.cpp provides a simple example of how an FM synthesizer can be implemented: float Fm::tick(){ int mIndex = mPhasor.tick()*SINE_TABLE_SIZE; float modulator = sineTable.tick(mIndex); cPhasor.setFrequency(cFreq + modulator*modIndex); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } Note that as for the AM example, we're saving an extra sine wave table by using the same one for both oscillators. The examples folder of the course repository hosts a simple Teensy program illustrating the use of FM. Use the Rec and Mode button to cycle through the parameters of the synth and change their value. FM synthesis was discovered in the late 1960s by John Chowning at Stanford University in California. He's now considered as one of the funding fathers of music technology and computer music. FM completely revolutionized the world of music in the 1980s by allowing Yamaha to produce the first commercial digital synthesizers: the DX7 which met a huge success. FM synthesis is the second most profitable patent that Stanford ever had.","title":"Frequency Modulation (FM) Synthesis"},{"location":"lectures/dsp1/#simple-filter-one-zero","text":"Filters are heavily used in the field of audio processing. In fact, designing filters is a whole field by itself. They are at the basis of many audio effects such as Wah guitar pedals, etc. From an algorithmic standpoint, the most basic filter is what we call a \"one zero\" filter which means that its transfer function only has numerators and no denominators. The differential equation of a one zero filter can be expressed as: y(n) = b_0x(n) + b_1x(n-1) where b_1 is \"the zero\" of the filter (also called feed forward coefficient ), b_0 can be discarded as it is equal to 1 in most cases. One zero filters can either be used as a lowpass if the value of b_1 is positive or as a highpass if b_1 is negative. The frequency response of the filter which is be obtained with H(e^{j \\omega T}) = b_0 + b_1e^{-j \\omega T} can be visualized on Julius Smith's website . Note that the gain of the signal is amplified on the second half of the spectrum which needs to be taken into account if this filter is used to process audio (once again, the output signal must be bounded within {-1,1}). OneZero.cpp implements a one zero filter: float OneZero::tick(float input){ float output = input + del*b1; del = input; return output*0.5; } Note that we multiply the output by 0.5 to normalize the output gain. The filtered-noise example program for the Teensy demonstrates the use of OneZero.cpp by feeding white noise in it. The value of b_1 can be changed by pressing the \"Mode\" button on the board, give it a try!","title":"Simple Filter: One Zero"},{"location":"lectures/dsp1/#exercises","text":"","title":"Exercises"},{"location":"lectures/dsp1/#lfo-low-frequency-oscillator","text":"An LFO is an oscillator whose frequency is below the human hearing range (20 Hz). LFOs are typically used to create vibrato. In that case, the frequency of the LFO is usually set to 6 Hz. Modify the crazy-saw example so that notes are played slower (1 per second) and that some vibrato is added to the generated sound. Solution: In MyDsp.h : #include \"Sine.h\" ... private: float freq; Phasor sawtooth; Echo echo; Sine LFO; }; In MyDsp.cpp : MyDsp::MyDsp() : ... freq(440), sawtooth(AUDIO_SAMPLE_RATE_EXACT), echo(AUDIO_SAMPLE_RATE_EXACT,10000), LFO(AUDIO_SAMPLE_RATE_EXACT) { ... // setting up DSP objects echo.setDel(10000); echo.setFeedback(0.5); LFO.setFrequency(6); } ... // set sine wave frequency void MyDsp::setFreq(float f){ freq = f; } ... for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { // DSP sawtooth.setFrequency(freq*(1 + LFO.tick()*0.1)); float currentSample = echo.tick(sawtooth.tick()*2 - 1)*0.5;","title":"LFO: Low Frequency Oscillator"},{"location":"lectures/dsp1/#towards-the-dx7","text":"The DX7 carried out frequency modulation over a total of six oscillators that could be patched in different ways . So FM is not limited to two oscillators... Try to implement an FM synthesizer involving 3 oscillators instead of one. They should be connected in series: 3 -> 2 -> 1. Solution: (non-exhaustive) In Fm.cpp : float Fm::tick(){ int m0Index = m0Phasor.tick()*SINE_TABLE_SIZE; float modulator0 = sineTable.tick(m0Index); modulator1.setFrequency(m1Freq + modulator0*mod0Index); int m1Index = m1Phasor.tick()*SINE_TABLE_SIZE; float modulator1 = sineTable.tick(m1Index); cPhasor.setFrequency(cFreq + modulator1*mod1Index); int cIndex = cPhasor.tick()*SINE_TABLE_SIZE; return sineTable.tick(cIndex)*gain; } And more exhaustive solution is provided in fm3 here .","title":"Towards the DX7"},{"location":"lectures/dsp1/#smoothing","text":"In most cases, DSP parameters are executed at control rate. Moreover, the resolution of the value used to configure parameters is much lower than that of audio samples since it might come from a Graphical User Interface (GUI), a low resolution sensor ADC (e.g., arduino), etc. For all these reasons, changing the value of a DSP parameter will often result in a \"click\"/discontinuity. A common way to prevent this from happening is to interpolate between the values of the parameter using a \"leaky integrator.\" In signal processing, this can be easily implemented using a normalized one pole lowpass filter: y(n) = (1-s)x(n) + sy(n-1) where s is the value of the pole and is typically set to 0.999 for optimal results. Modify the crazy-saw example by \"smoothing\" the value of the frequency parameter by implementing the filter above with s=0.999 . Then slow down the rate at which frequency is being changed so that only two new values are generated per second. The result should sound quite funny :). Solution: In addition to Smooth.cpp and Smooth.h , in Phasor.h : int samplingRate; Smooth smooth; }; and Phasor.cpp : float Phasor::tick(){ float currentSample = phasor; phasor += smooth.tick(phasorDelta); phasor = phasor - std::floor(phasor); return currentSample; }","title":"Smoothing"},{"location":"lectures/dsp1/#smoothing-potentiometer-values","text":"Try to use the smoothing function that you implemented in the previous step to smooth sensor values coming from a potential potentiometer controlling some parameter of one of the Teensy examples. The main idea is to get rid of sound artifacts when making abrupt changes in potentiometers.","title":"Smoothing Potentiometer Values"},{"location":"lectures/dsp2/","text":"Audio Processing Basics II Harmonic Distortion: Rock On! Distortion is one of the most common electric guitar effect. It consists of over driving a signal by increasing its gain to \"square\" the extremities of its waveform. This results in the creation of lots of harmonics, producing very \"rich\" sounds. Overdrive is easily achievable with an analog electronic circuit and \"sharp edges\" in the waveform are rounded thanks to the tolerance of the electronic components. In the digital world, things are slightly more complicated since clipping will happen resulting in a very dirty sound with potentially lots of aliasing. One way to solve this problem is to use a \"cubic function\" which will round the edges of the signal above a certain amplitude: f(x) = \\begin{cases} \\frac{-2}{3}, \\; \\; x \\leq -1\\\\ x - \\frac{x^3}{3}, \\; \\; -1 < x < 1\\\\ \\frac{2}{3}, \\; \\; x \\geq -1 \\end{cases} Distortion.cpp implements a cubic distortion as: float Distortion::cubic(float x){ return x - x*x*x/3; } float Distortion::tick(float input){ float output = input*pow(10.0,2*drive) + offset; output = fmax(-1,fmin(1,output)); output = cubic(output); return output*gain; } The range of drive is {0;1} which means that the value of input can be multiplied by a number as great as 100 here. offset is a common parameter which just adds a positive or negative DC offset to the signal. If this parameter is used, it is recommended to add a DC blocking filter after the distortion. Distortion is created here by clipping the signal using the fmin and fmax functions. Finally, the cubic polynomial is used to round the edges of the waveform of the signal as explained above. The distortion example program for the Teensy demonstrates the use of Distortion.cpp . Distortion is a very trendy field of research in audio technology these days especially using \"virtual analog\" algorithms which consists of modeling the electronic circuit of distortion on a computer. Echo An echo is a very common audio effect which is used a lot to add some density and depth to a sound. It is based on a feedback loop and a delay and can be expressed as: y(n) = x(n) + g.y(n - M) where g is the feedback between 0 and 1 and M the delay as a number of samples. It can be seen as a simple physical model of what happens in the real world when echo is produced: the delay represents the time it takes for an acoustical wave to go from point A to point B at the speed of sound and g can control the amount of absorption created by the air and the reflecting material. Echo.cpp implements an echo as: float Echo::tick(float input){ float output = input + delBuffer[readIndex]*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } Here, delBuffer is used as a \"ring buffer\": incoming samples are stored and the read and write indices loop around to buffer to write incoming samples and read previous ones. Note that memory is allocated in the constructor of the class for delBuffer based on the value of maxDel , the maximum size of the delay. The echo example program for the Teensy demonstrates the use of Echo.cpp . Comb A comb filter is a filter whose frequency response looks like a \"comb.\" Comb filters can be implemented with feed-forward filters (Finite Impulse Response -- FIR) or feedback filters (Infinite Impulse Response -- IIR). In fact, the Echo algorithm can be used as a comb filter if the delay is very short: y(n) = x(n)-g.y(n-M) where M is the length of the delay and g feedback coefficient. Julius Smith's website presents the frequency response of such filter and the mathematical rationals behind it. From an acoustical standpoint, a feedback comb filter will introduce resonances at specific point in the spectrum of the sound. The position and the spacing of these resonances is determined by the value of M . g , on the other hand, will determine the amplitude and sharpness of these resonances. The comb example program for the Teensy demonstrates the use of Echo.cpp as a comb filter. The \"Mode\" button can be used to change the value of the delay. Physical Modeling: the Simple Case of the Karplus Strong Physical modeling is one of the most advanced sound synthesis technique and a very active field of research. It consists of using physics/mathematical models of musical instruments or vibrating structures to synthesize sound. Various physical modeling techniques are used in the field of audio synthesis: Mass/Interaction (MI), Finite Difference Scheme (FDS), Signal models (e.g., waveguides, modal systems, etc.). While MI and FDS model the vibrational behavior of a system (i.e., using partial differential equation in the case of FDS), signal models model an object as a combination of signal processors. In this section, we will only look at this type of model the other ones being out of the scope of this class. An extremely primitive string model can be implemented using a delay line and a loop. The delay line models the time it takes for vibration in the string to go from one extremity to the other, and the loop models the reflections at the boundaries of the string. In other words, we can literally just reuse the echo algorithm for this. This primitive string model is called the \"Karplus-Strong\" algorithm: Karplus-Strong Algorithm (Source: Wikipedia ) The Karplus-Strong algorithm is typically implemented as: y(n) = x(n) + \\alpha\\frac{y(n-L) + y(n-L-1)}{2} where: x(n) is the input signal (typically an dirac or a noise burst), \\alpha is the feedback coefficient (or dispersion coefficient, in that case), L is the length of the delay and hence, the length of the string. \\frac{y(n-L) + y(n-L-1)}{2} can be seen as a one zero filter implementing a lowpass. It models the fact that high frequencies are absorbed faster than low frequencies at the extremities of a string. The length of the delay L can be controlled as a frequency using the following formula: L = fs/f where f is the desired frequency. At the very least, the system must be excited by a dirac (i.e., a simple impulse going from 1 to 0). The quality of the generated sound can be significantly improved if a noise impulse is used though. KS.cpp implements a basic Karplus-Strong algorithm: float KS::tick(){ float excitation; if(trig){ excitation = 1.0; trig = false; } else{ excitation = 0.0; } float output = excitation + oneZero(delBuffer[readIndex])*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } with: float KS::oneZero(float x){ float output = (x + zeroDel)*0.5; zeroDel = output; return output; } The examples folder of the course repository hosts a simple Teensy program illustrating the use of KS.cpp . Note that this algorithm could be improved in many ways. In particular, the fact that the delay length is currently expressed as an integer can result in frequency mismatches at high frequencies. In other words, our current string is out of tune. This could be fixed using fractional delay . In practice, the Karplus-Strong algorithm is not a physical model per se and is just a simplification of the ideal string wave equation . More advanced signal models can be implemented using waveguides. Waveguide physical modeling has been extensively used in modern synthesizers to synthesize the sound of acoustic instruments. Julius O. Smith (Stanford professor) is the father of waveguide physical modeling. Exercises Making Resonant Lowpass, Bandpass and Highpass This short tutorial demonstrates how to implement a series of filters that can be configured as resonant lowpass, bandpass, and highpasses. For this, you will first need to implement a biquad filter: https://en.wikipedia.org/wiki/Digital_biquad_filter (direct form 2 is preferred). You will then have to format the coefficients of that filter using the bilinear transform such that: tf2s(b2,b1,b0,a1,a0,w1) = tf2(b0d,b1d,b2d,a1d,a2d) with { c = 1/tan(w1*0.5/SR); csq = c*c; d = a0 + a1 * c + csq; b0d = (b0 + b1 * c + b2 * csq)/d; b1d = 2 * (b0 - b2 * csq)/d; b2d = (b0 - b1 * c + b2 * csq)/d; a1d = 2 * (a0 - csq)/d; a2d = (a0 - a1*c + csq)/d; }; where tf2 is a direct form 2 biquad and SR the sampling rate. Finally, you'll have to format the coefficients of the tf2s filter such that: resonlp(fc,Q,gain) = tf2s(b2,b1,b0,a1,a0,wc) with { wc = 2*PI*fc; a1 = 1/Q; a0 = 1; b2 = 0; b1 = 0; b0 = gain; }; (for the resonant lowpass) resonbp(fc,Q,gain) = tf2s(b2,b1,b0,a1,a0,wc) with { wc = 2*PI*fc; a1 = 1/Q; a0 = 1; b2 = 0; b1 = gain; b0 = 0; }; (for the resonant bandpass) resonhp(fc,Q,gain,x) = gain*x-resonlp(fc,Q,gain,x); (for the resonant highpass). Please, note that Q controls the bandwidth of the filter such that: Q = fc/BW . Wrap this up by plugging a broadband signal generator (e.g., sawtooth oscillator or white noise generator) to the filter. Come up with some nice mapping controlled with hardware sensors (i.e., rotary pot, etc.). Test your filter by changing dynamically the cutoff frequency of the filter using a potentiometer, for example. Peak Equalizers Peak equalizers are yet another kind of filters allowing to reduce or increase some bands in the spectrum of a sound. A peak equalizer can take the following form: peak_eq(Lfx,fx,B) = tf2s(1,b1s,1,a1s,1,wx) with { T = 1.0/SR; Bw = B*T/sin(wx*T); // prewarp s-bandwidth for more accuracy in z-plane a1 = PI*Bw; b1 = g*a1; g = db2linear(abs(Lfx)); if(Lfx>0) { b1s = b1; a1s = a1; } else { b1s = a1; a1s = b1; } wx = 2*PI*fx; }; where the definition of tf2s (direct-form 2 biquadratic filter operating the bilinear transform) can be found above. Lfx controls the level of the filter in dB (0 for no filtering, negative value for band reduction, and positive value for band amplification). fx is the center frequency, B the bandwidth in Hz. Implement this filter and test it the same way as you did for the resonant lowpass/bandpass/highpass. Browsing Through the Teensy Audio Library Examples The Teensy audio library comes with a series of example programs which are pre-installed with Teensyduino in File/Examples/Teensy/Audio (in the Teensyduino interface). Browse through the examples to get a sense of what's out there.","title":" 7: Audio Processing Basics II "},{"location":"lectures/dsp2/#audio-processing-basics-ii","text":"","title":"Audio Processing Basics II"},{"location":"lectures/dsp2/#harmonic-distortion-rock-on","text":"Distortion is one of the most common electric guitar effect. It consists of over driving a signal by increasing its gain to \"square\" the extremities of its waveform. This results in the creation of lots of harmonics, producing very \"rich\" sounds. Overdrive is easily achievable with an analog electronic circuit and \"sharp edges\" in the waveform are rounded thanks to the tolerance of the electronic components. In the digital world, things are slightly more complicated since clipping will happen resulting in a very dirty sound with potentially lots of aliasing. One way to solve this problem is to use a \"cubic function\" which will round the edges of the signal above a certain amplitude: f(x) = \\begin{cases} \\frac{-2}{3}, \\; \\; x \\leq -1\\\\ x - \\frac{x^3}{3}, \\; \\; -1 < x < 1\\\\ \\frac{2}{3}, \\; \\; x \\geq -1 \\end{cases} Distortion.cpp implements a cubic distortion as: float Distortion::cubic(float x){ return x - x*x*x/3; } float Distortion::tick(float input){ float output = input*pow(10.0,2*drive) + offset; output = fmax(-1,fmin(1,output)); output = cubic(output); return output*gain; } The range of drive is {0;1} which means that the value of input can be multiplied by a number as great as 100 here. offset is a common parameter which just adds a positive or negative DC offset to the signal. If this parameter is used, it is recommended to add a DC blocking filter after the distortion. Distortion is created here by clipping the signal using the fmin and fmax functions. Finally, the cubic polynomial is used to round the edges of the waveform of the signal as explained above. The distortion example program for the Teensy demonstrates the use of Distortion.cpp . Distortion is a very trendy field of research in audio technology these days especially using \"virtual analog\" algorithms which consists of modeling the electronic circuit of distortion on a computer.","title":"Harmonic Distortion: Rock On!"},{"location":"lectures/dsp2/#echo","text":"An echo is a very common audio effect which is used a lot to add some density and depth to a sound. It is based on a feedback loop and a delay and can be expressed as: y(n) = x(n) + g.y(n - M) where g is the feedback between 0 and 1 and M the delay as a number of samples. It can be seen as a simple physical model of what happens in the real world when echo is produced: the delay represents the time it takes for an acoustical wave to go from point A to point B at the speed of sound and g can control the amount of absorption created by the air and the reflecting material. Echo.cpp implements an echo as: float Echo::tick(float input){ float output = input + delBuffer[readIndex]*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } Here, delBuffer is used as a \"ring buffer\": incoming samples are stored and the read and write indices loop around to buffer to write incoming samples and read previous ones. Note that memory is allocated in the constructor of the class for delBuffer based on the value of maxDel , the maximum size of the delay. The echo example program for the Teensy demonstrates the use of Echo.cpp .","title":"Echo"},{"location":"lectures/dsp2/#comb","text":"A comb filter is a filter whose frequency response looks like a \"comb.\" Comb filters can be implemented with feed-forward filters (Finite Impulse Response -- FIR) or feedback filters (Infinite Impulse Response -- IIR). In fact, the Echo algorithm can be used as a comb filter if the delay is very short: y(n) = x(n)-g.y(n-M) where M is the length of the delay and g feedback coefficient. Julius Smith's website presents the frequency response of such filter and the mathematical rationals behind it. From an acoustical standpoint, a feedback comb filter will introduce resonances at specific point in the spectrum of the sound. The position and the spacing of these resonances is determined by the value of M . g , on the other hand, will determine the amplitude and sharpness of these resonances. The comb example program for the Teensy demonstrates the use of Echo.cpp as a comb filter. The \"Mode\" button can be used to change the value of the delay.","title":"Comb"},{"location":"lectures/dsp2/#physical-modeling-the-simple-case-of-the-karplus-strong","text":"Physical modeling is one of the most advanced sound synthesis technique and a very active field of research. It consists of using physics/mathematical models of musical instruments or vibrating structures to synthesize sound. Various physical modeling techniques are used in the field of audio synthesis: Mass/Interaction (MI), Finite Difference Scheme (FDS), Signal models (e.g., waveguides, modal systems, etc.). While MI and FDS model the vibrational behavior of a system (i.e., using partial differential equation in the case of FDS), signal models model an object as a combination of signal processors. In this section, we will only look at this type of model the other ones being out of the scope of this class. An extremely primitive string model can be implemented using a delay line and a loop. The delay line models the time it takes for vibration in the string to go from one extremity to the other, and the loop models the reflections at the boundaries of the string. In other words, we can literally just reuse the echo algorithm for this. This primitive string model is called the \"Karplus-Strong\" algorithm: Karplus-Strong Algorithm (Source: Wikipedia ) The Karplus-Strong algorithm is typically implemented as: y(n) = x(n) + \\alpha\\frac{y(n-L) + y(n-L-1)}{2} where: x(n) is the input signal (typically an dirac or a noise burst), \\alpha is the feedback coefficient (or dispersion coefficient, in that case), L is the length of the delay and hence, the length of the string. \\frac{y(n-L) + y(n-L-1)}{2} can be seen as a one zero filter implementing a lowpass. It models the fact that high frequencies are absorbed faster than low frequencies at the extremities of a string. The length of the delay L can be controlled as a frequency using the following formula: L = fs/f where f is the desired frequency. At the very least, the system must be excited by a dirac (i.e., a simple impulse going from 1 to 0). The quality of the generated sound can be significantly improved if a noise impulse is used though. KS.cpp implements a basic Karplus-Strong algorithm: float KS::tick(){ float excitation; if(trig){ excitation = 1.0; trig = false; } else{ excitation = 0.0; } float output = excitation + oneZero(delBuffer[readIndex])*feedback; delBuffer[writeIndex] = output; readIndex = (readIndex+1)%del; writeIndex = (writeIndex+1)%del; return output; } with: float KS::oneZero(float x){ float output = (x + zeroDel)*0.5; zeroDel = output; return output; } The examples folder of the course repository hosts a simple Teensy program illustrating the use of KS.cpp . Note that this algorithm could be improved in many ways. In particular, the fact that the delay length is currently expressed as an integer can result in frequency mismatches at high frequencies. In other words, our current string is out of tune. This could be fixed using fractional delay . In practice, the Karplus-Strong algorithm is not a physical model per se and is just a simplification of the ideal string wave equation . More advanced signal models can be implemented using waveguides. Waveguide physical modeling has been extensively used in modern synthesizers to synthesize the sound of acoustic instruments. Julius O. Smith (Stanford professor) is the father of waveguide physical modeling.","title":"Physical Modeling: the Simple Case of the Karplus Strong"},{"location":"lectures/dsp2/#exercises","text":"","title":"Exercises"},{"location":"lectures/dsp2/#making-resonant-lowpass-bandpass-and-highpass","text":"This short tutorial demonstrates how to implement a series of filters that can be configured as resonant lowpass, bandpass, and highpasses. For this, you will first need to implement a biquad filter: https://en.wikipedia.org/wiki/Digital_biquad_filter (direct form 2 is preferred). You will then have to format the coefficients of that filter using the bilinear transform such that: tf2s(b2,b1,b0,a1,a0,w1) = tf2(b0d,b1d,b2d,a1d,a2d) with { c = 1/tan(w1*0.5/SR); csq = c*c; d = a0 + a1 * c + csq; b0d = (b0 + b1 * c + b2 * csq)/d; b1d = 2 * (b0 - b2 * csq)/d; b2d = (b0 - b1 * c + b2 * csq)/d; a1d = 2 * (a0 - csq)/d; a2d = (a0 - a1*c + csq)/d; }; where tf2 is a direct form 2 biquad and SR the sampling rate. Finally, you'll have to format the coefficients of the tf2s filter such that: resonlp(fc,Q,gain) = tf2s(b2,b1,b0,a1,a0,wc) with { wc = 2*PI*fc; a1 = 1/Q; a0 = 1; b2 = 0; b1 = 0; b0 = gain; }; (for the resonant lowpass) resonbp(fc,Q,gain) = tf2s(b2,b1,b0,a1,a0,wc) with { wc = 2*PI*fc; a1 = 1/Q; a0 = 1; b2 = 0; b1 = gain; b0 = 0; }; (for the resonant bandpass) resonhp(fc,Q,gain,x) = gain*x-resonlp(fc,Q,gain,x); (for the resonant highpass). Please, note that Q controls the bandwidth of the filter such that: Q = fc/BW . Wrap this up by plugging a broadband signal generator (e.g., sawtooth oscillator or white noise generator) to the filter. Come up with some nice mapping controlled with hardware sensors (i.e., rotary pot, etc.). Test your filter by changing dynamically the cutoff frequency of the filter using a potentiometer, for example.","title":"Making Resonant Lowpass, Bandpass and Highpass"},{"location":"lectures/dsp2/#peak-equalizers","text":"Peak equalizers are yet another kind of filters allowing to reduce or increase some bands in the spectrum of a sound. A peak equalizer can take the following form: peak_eq(Lfx,fx,B) = tf2s(1,b1s,1,a1s,1,wx) with { T = 1.0/SR; Bw = B*T/sin(wx*T); // prewarp s-bandwidth for more accuracy in z-plane a1 = PI*Bw; b1 = g*a1; g = db2linear(abs(Lfx)); if(Lfx>0) { b1s = b1; a1s = a1; } else { b1s = a1; a1s = b1; } wx = 2*PI*fx; }; where the definition of tf2s (direct-form 2 biquadratic filter operating the bilinear transform) can be found above. Lfx controls the level of the filter in dB (0 for no filtering, negative value for band reduction, and positive value for band amplification). fx is the center frequency, B the bandwidth in Hz. Implement this filter and test it the same way as you did for the resonant lowpass/bandpass/highpass.","title":"Peak Equalizers"},{"location":"lectures/dsp2/#browsing-through-the-teensy-audio-library-examples","text":"The Teensy audio library comes with a series of example programs which are pre-installed with Teensyduino in File/Examples/Teensy/Audio (in the Teensyduino interface). Browse through the examples to get a sense of what's out there.","title":"Browsing Through the Teensy Audio Library Examples"},{"location":"lectures/faust-teensy/","text":"Faust on the Teensy and Advanced Control Generating and Using a Faust C++ Object In order to run the examples in this lecture, you should install the Faust distribution on your system from the Faust Git Repository . At the most fundamental level, the Faust compiler is a command line tool translating a Faust DSP object into C++ code. For example, assuming that Faust is properly installed on your system, given the following simple Faust program implementing a filtered sawtooth wave oscillator ( FaustSynth.dsp ): import(\"stdfaust.lib\"); freq = nentry(\"freq\",200,50,1000,0.01); gain = nentry(\"gain\",0.5,0,1,0.01) : si.smoo; gate = button(\"gate\") : si.smoo; cutoff = nentry(\"cutoff\",10000,50,10000,0.01) : si.smoo; process = os.sawtooth(freq)*gain*gate : fi.lowpass(3,cutoff) <: _,_; running: faust FaustSynth.dsp will output the C++ code corresponding to this file in the terminal. Faust comes with a system of C++ wrapper (called architectures in the Faust ecosystem) which can be used to customize the generated C++ code. faustMinimal.h is a minimal architecture file including some C++ objects that can be used to facilitate interactions with the generated DSP: #include <cmath> #include <cstring> #include \"faust/gui/MapUI.h\" #include \"faust/gui/meta.h\" #include \"faust/dsp/dsp.h\" // BEGIN-FAUSTDSP <<includeIntrinsic>> <<includeclass>> // END-FAUSTDSP For instance, MapUI allows us to access the parameters of a Faust DSP object using the setParamValue method, etc. To generate a C++ file using this architecture, you can run: faust -i -a faustMinimal.h FaustSynth.dsp -o FaustSynth.h which will produce a FaustSynth.h file (feel free to click on it). The -i inlines all the included C++ .h files in the generated file. The faust-synth Teensy example project demonstrates how FaustSynth.h can be used. First, it is included in MyDsp.cpp and the following elements are declared in the corresponding header file : private: MapUI* fUI; dsp* fDSP; float **outputs; dsp is the actual Faust DSP, MapUI will be used to interact with it, and outputs is the multidimensional output buffer. These objects are then allocated in the constructor of MyDsp.cpp : fDSP = new mydsp(); fDSP->init(AUDIO_SAMPLE_RATE_EXACT); fUI = new MapUI(); fDSP->buildUserInterface(fUI); outputs = new float*[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; ++channel){ outputs[channel] = new float[AUDIO_BLOCK_SAMPLES]; } buildUserInterface is used to connect fUI to fDSP and then memory is allocated for the output buffer. Note that memory should be de-allocated in the destructor after this. In the update method, we just call the compute method of fDSP and then reformat the generated samples to transmit them via i2s: void MyDsp::update(void) { fDSP->compute(AUDIO_BLOCK_SAMPLES,NULL,outputs); audio_block_t* outBlock[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; channel++) { outBlock[channel] = allocate(); if (outBlock[channel]) { for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { int16_t val = outputs[channel][i]*MULT_16; outBlock[channel]->data[i] = val; } transmit(outBlock[channel], channel); release(outBlock[channel]); } } } Note that outputs is used as an intermediate here and the first dimension of the array is the channel number and the second dimension the samples themselves. The various parameters of the Faust object can then be changed just by calling the setParamValue method. The first argument of the method corresponds to the name of the parameter as specified in the Faust program: void MyDsp::setFreq(float freq){ fUI->setParamValue(\"freq\",freq); } void MyDsp::setCutoff(float freq){ fUI->setParamValue(\"cutoff\",freq); } void MyDsp::setGate(int gate){ fUI->setParamValue(\"gate\",gate); } Better Control on the Teensy In addition to controlling DSP parameters on the Teensy using external sensors connected to the board's GPIOs, other techniques can potentially be used. We briefly summarize this section. MIDI MIDI is THE standard in the world of music to control digital devices. It has been around since 1983 and even though it is very \"low tech,\" it is still heavily used. While MIDI was traditionally transmitted over MIDI ports, USB is used nowadays to send MIDI. USB MIDI is natively supported on the Teensy through the Teensy USB MIDI library: https://www.pjrc.com/teensy/td_midi.html . Interfacing this library with your DSP programs should be very straightforward. Please, also note that Teensys can be used to send MIDI messages over USB which means that implementing your own midi controller using a Teensy is fairly straightforward as well. If you're curious about this, you can check this page: https://ccrma.stanford.edu/courses/250a-winter-2018/labs/2/ . OSC Open Sound Control (OSC) is a more modern communication standard used in the field of music technology. It is based on UDP which means that information can be transmitted via Ethernet or Wi-Fi. OSC uses a system of address/values to access the different parameters of a system. An OSC message can therefore look like: /synth/freq 440 On the Teensy, dealing with OSC is a bit more tricky than MIDI because the Teensy 4.0 provided as part of your class kit don't have a built-in Ethernet port. Hence, the only way to get an Ethernet connection to the Teensy is to buy an external Ethernet adapter (that will likely connect to the Teensy through i2c, etc.). Another option is to buy a Teensy 4.1 which hosts an Ethernet chip (an Ethernet connector can just be soldered to the board). Exercises Faust Triangle Oscillator The Faust libraries host a triangle wave oscillator: os.triangle Try to replace the sawtooth wave oscillator from the previous example by a triangle wave oscillator in Faust and run it on the Teensy. Flanger The Faust libraries host a flanger function : pf.flanger_mono Turn your Teensy into a flanger effect processor!","title":" 8: Faust on the Teensy and Advanced Control "},{"location":"lectures/faust-teensy/#faust-on-the-teensy-and-advanced-control","text":"","title":"Faust on the Teensy and Advanced Control"},{"location":"lectures/faust-teensy/#generating-and-using-a-faust-c-object","text":"In order to run the examples in this lecture, you should install the Faust distribution on your system from the Faust Git Repository . At the most fundamental level, the Faust compiler is a command line tool translating a Faust DSP object into C++ code. For example, assuming that Faust is properly installed on your system, given the following simple Faust program implementing a filtered sawtooth wave oscillator ( FaustSynth.dsp ): import(\"stdfaust.lib\"); freq = nentry(\"freq\",200,50,1000,0.01); gain = nentry(\"gain\",0.5,0,1,0.01) : si.smoo; gate = button(\"gate\") : si.smoo; cutoff = nentry(\"cutoff\",10000,50,10000,0.01) : si.smoo; process = os.sawtooth(freq)*gain*gate : fi.lowpass(3,cutoff) <: _,_; running: faust FaustSynth.dsp will output the C++ code corresponding to this file in the terminal. Faust comes with a system of C++ wrapper (called architectures in the Faust ecosystem) which can be used to customize the generated C++ code. faustMinimal.h is a minimal architecture file including some C++ objects that can be used to facilitate interactions with the generated DSP: #include <cmath> #include <cstring> #include \"faust/gui/MapUI.h\" #include \"faust/gui/meta.h\" #include \"faust/dsp/dsp.h\" // BEGIN-FAUSTDSP <<includeIntrinsic>> <<includeclass>> // END-FAUSTDSP For instance, MapUI allows us to access the parameters of a Faust DSP object using the setParamValue method, etc. To generate a C++ file using this architecture, you can run: faust -i -a faustMinimal.h FaustSynth.dsp -o FaustSynth.h which will produce a FaustSynth.h file (feel free to click on it). The -i inlines all the included C++ .h files in the generated file. The faust-synth Teensy example project demonstrates how FaustSynth.h can be used. First, it is included in MyDsp.cpp and the following elements are declared in the corresponding header file : private: MapUI* fUI; dsp* fDSP; float **outputs; dsp is the actual Faust DSP, MapUI will be used to interact with it, and outputs is the multidimensional output buffer. These objects are then allocated in the constructor of MyDsp.cpp : fDSP = new mydsp(); fDSP->init(AUDIO_SAMPLE_RATE_EXACT); fUI = new MapUI(); fDSP->buildUserInterface(fUI); outputs = new float*[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; ++channel){ outputs[channel] = new float[AUDIO_BLOCK_SAMPLES]; } buildUserInterface is used to connect fUI to fDSP and then memory is allocated for the output buffer. Note that memory should be de-allocated in the destructor after this. In the update method, we just call the compute method of fDSP and then reformat the generated samples to transmit them via i2s: void MyDsp::update(void) { fDSP->compute(AUDIO_BLOCK_SAMPLES,NULL,outputs); audio_block_t* outBlock[AUDIO_OUTPUTS]; for (int channel = 0; channel < AUDIO_OUTPUTS; channel++) { outBlock[channel] = allocate(); if (outBlock[channel]) { for (int i = 0; i < AUDIO_BLOCK_SAMPLES; i++) { int16_t val = outputs[channel][i]*MULT_16; outBlock[channel]->data[i] = val; } transmit(outBlock[channel], channel); release(outBlock[channel]); } } } Note that outputs is used as an intermediate here and the first dimension of the array is the channel number and the second dimension the samples themselves. The various parameters of the Faust object can then be changed just by calling the setParamValue method. The first argument of the method corresponds to the name of the parameter as specified in the Faust program: void MyDsp::setFreq(float freq){ fUI->setParamValue(\"freq\",freq); } void MyDsp::setCutoff(float freq){ fUI->setParamValue(\"cutoff\",freq); } void MyDsp::setGate(int gate){ fUI->setParamValue(\"gate\",gate); }","title":"Generating and Using a Faust C++ Object"},{"location":"lectures/faust-teensy/#better-control-on-the-teensy","text":"In addition to controlling DSP parameters on the Teensy using external sensors connected to the board's GPIOs, other techniques can potentially be used. We briefly summarize this section.","title":"Better Control on the Teensy"},{"location":"lectures/faust-teensy/#midi","text":"MIDI is THE standard in the world of music to control digital devices. It has been around since 1983 and even though it is very \"low tech,\" it is still heavily used. While MIDI was traditionally transmitted over MIDI ports, USB is used nowadays to send MIDI. USB MIDI is natively supported on the Teensy through the Teensy USB MIDI library: https://www.pjrc.com/teensy/td_midi.html . Interfacing this library with your DSP programs should be very straightforward. Please, also note that Teensys can be used to send MIDI messages over USB which means that implementing your own midi controller using a Teensy is fairly straightforward as well. If you're curious about this, you can check this page: https://ccrma.stanford.edu/courses/250a-winter-2018/labs/2/ .","title":"MIDI"},{"location":"lectures/faust-teensy/#osc","text":"Open Sound Control (OSC) is a more modern communication standard used in the field of music technology. It is based on UDP which means that information can be transmitted via Ethernet or Wi-Fi. OSC uses a system of address/values to access the different parameters of a system. An OSC message can therefore look like: /synth/freq 440 On the Teensy, dealing with OSC is a bit more tricky than MIDI because the Teensy 4.0 provided as part of your class kit don't have a built-in Ethernet port. Hence, the only way to get an Ethernet connection to the Teensy is to buy an external Ethernet adapter (that will likely connect to the Teensy through i2c, etc.). Another option is to buy a Teensy 4.1 which hosts an Ethernet chip (an Ethernet connector can just be soldered to the board).","title":"OSC"},{"location":"lectures/faust-teensy/#exercises","text":"","title":"Exercises"},{"location":"lectures/faust-teensy/#faust-triangle-oscillator","text":"The Faust libraries host a triangle wave oscillator: os.triangle Try to replace the sawtooth wave oscillator from the previous example by a triangle wave oscillator in Faust and run it on the Teensy.","title":"Faust Triangle Oscillator"},{"location":"lectures/faust-teensy/#flanger","text":"The Faust libraries host a flanger function : pf.flanger_mono Turn your Teensy into a flanger effect processor!","title":"Flanger"},{"location":"lectures/faust/","text":"Introduction to Faust Useful Resources Faust Website: https://faust.grame.fr Faust Documentation: https://faustdoc.grame.fr Faust Libraries Documentation: https://faustlibraries.grame.fr Faust Online IDE: https://faustide.grame.fr All programs below can be executed in the Faust Online IDE, give them a try ;). Sawtooth Synth import(\"stdfaust.lib\"); f = hslider(\"freq\",400,50,2000,0.01); g = hslider(\"gain\",1,0,1,0.01); t = button(\"gate\"); envelope = t : en.adsr(0.1,0.01,0.8,0.5); process = os.sawtooth(f)*g*envelope <: _,_; effect = dm.zita_light; Panner import(\"stdfaust.lib\"); g = hslider(\"gain\",1,0,1,0.01) : si.smoo; pan(p) = _ <: _*(1-psqrt),_*psqrt with{ psqrt = p : sqrt; }; process = pan(g); Oscillator From Scratch import(\"stdfaust.lib\"); saw(f) = (A~B)/p*2-1 with{ p = ma.SR/f; A = _; B = (_+1)%p; }; sine(f) = (saw(f)*0.5 + 0.5)*2*ma.PI : sin; freq = hslider(\"freq[acc: 0 0 -10 0 10]\",400,50,2000,0.01) : si.smoo; s = checkbox(\"switch\"); process = sine(freq)*(s==0) + saw(freq)*(s==1);","title":" 5: Introduction to Faust "},{"location":"lectures/faust/#introduction-to-faust","text":"","title":"Introduction to Faust"},{"location":"lectures/faust/#useful-resources","text":"Faust Website: https://faust.grame.fr Faust Documentation: https://faustdoc.grame.fr Faust Libraries Documentation: https://faustlibraries.grame.fr Faust Online IDE: https://faustide.grame.fr All programs below can be executed in the Faust Online IDE, give them a try ;).","title":"Useful Resources"},{"location":"lectures/faust/#sawtooth-synth","text":"import(\"stdfaust.lib\"); f = hslider(\"freq\",400,50,2000,0.01); g = hslider(\"gain\",1,0,1,0.01); t = button(\"gate\"); envelope = t : en.adsr(0.1,0.01,0.8,0.5); process = os.sawtooth(f)*g*envelope <: _,_; effect = dm.zita_light;","title":"Sawtooth Synth"},{"location":"lectures/faust/#panner","text":"import(\"stdfaust.lib\"); g = hslider(\"gain\",1,0,1,0.01) : si.smoo; pan(p) = _ <: _*(1-psqrt),_*psqrt with{ psqrt = p : sqrt; }; process = pan(g);","title":"Panner"},{"location":"lectures/faust/#oscillator-from-scratch","text":"import(\"stdfaust.lib\"); saw(f) = (A~B)/p*2-1 with{ p = ma.SR/f; A = _; B = (_+1)%p; }; sine(f) = (saw(f)*0.5 + 0.5)*2*ma.PI : sin; freq = hslider(\"freq[acc: 0 0 -10 0 10]\",400,50,2000,0.01) : si.smoo; s = checkbox(\"switch\"); process = sine(freq)*(s==0) + saw(freq)*(s==1);","title":"Oscillator From Scratch"},{"location":"lectures/setup/","text":"Programming Environment Setup The Teensy 4.0 microcontroller platform is used throughout this course. The goal of this first session is to teach you how to program the Teensy from your own computer. The Teensy 4.0 and Audio Shield When combined together, the Teensy 4.0 and its companion audio adapter (see figure below) constitute a great platform for embedded real-time audio signal processing providing a stereo audio input and output, interfacing possibilities with sensors, etc. The Teensy 4.0 (left) and the associated audio adapter board (right) The Teensy 4.0 is a microcontroller-based development board developed by PJRC. As an open source environment, tons of useful information can be found on their website: https://www.pjrc.com . The Teensy 4.0 is based on a powerful ARM Cortex-M7 microcontroller providing plenty of computational power (600 MHz with a Floating Point Unit - FPU) for real-time audio DSP (Digital Signal Processing) and many GPIOs (General Purpose Inputs and Outputs) for interfacing, etc. The CPU performances are many times faster than typical 32 bit microcontrollers. The FPU performs 32 bit float and 64 bit double precision math directly in hardware. DSP extension instructions accelerate signal processing, filters, Fourier transform, etc. The Teensy Audio library automatically makes use of these DSP instructions. Teensy performance from Core Benchmarks This pinout reference card comes with the Teensy 4.0 and should be part of your kit ( do not loose it! ). IMPORTANT: The pins are not 5V tolerant. Do not drive any digital pin higher than 3.3V. Teensy 4.0 pin map (from the PJRC webite) The Teensy 4.0 has a total of 40 input/output signal pins. 24 are easily accessible when used with a solderless breadboard. The available pins include general purpose IO (i.e., GPIO, digital or analog), as well as integrated serial protocols (i.e., I2C, I2S, CAN, SPI, and UART protocols) that are used to connect to other devices. In this course, we use the audio adaptor board provided by PJRC that integrates a low power stereo audio codec (NXP Semiconductors SGTL5000 codec) and a SD card reader. The Teensy audio adaptor (rev. D) The audio codec connects to Teensy using 7 signals (Yellow signal in pin map above) which are used by two protocols: I2C and I2S. This is a traditional configuration for audio codecs: the I2C (or I\u00b2C: Inter-Integrated Circuit) protocol is used to configure the codec (i.e., sample rate, input and output pins, etc.) and the I2S (or I\u00b2S: Inter-IC Sound) is used to transfer samples bit by bit in both direction (i.e., from and to the teensy). The I2C pins SDA and SCL are used to control the chip and adjust parameters. Audio data uses I2S signals, TX (to headphones and/or line out) and RX (from line in or mic), and 3 clocks, LRCLK (44.1 kHz), BCLK (1.41 MHz) and MCLK (11.29 MHz). All 3 clocks are created by Teensy which means that the SGTL5000 operates in \"slave mode.\" The schematics of the audio shield board, rev. D, can bee seen here and the schematic of the Teensy 4.0 can be seen at the end of this page: https://www.pjrc.com/store/teensy40.html . Of course, as they are both made by PJRC, they are designed to be compatible. The USB connector of the Teensy can support many serial communication from the host computer to the Teensy: (i.e., JTAG for flashing/programming, Serial UART, midi, mouse, etc. see Tools -> USB Type menu in arduino IDE). In this course, the USB connector is used to program the device (i.e., download binary code into flash memory) and ascii communication between the host and the Teensy (i.e., using UART/Serial communicatino protocol). In linux machines, when the teensy USB cable is connected, the serial port will appear as /dev/ttyACM0 . The Teensy 4.0 has 2 Mbyte of flash memory intended for storing your code. 1Mbyte of memory is available for execution (i.e., for variables and data storing during execution). Half of this memory (RAM1) is accessed as tightly coupled memory for maximum performance. The other half (RAM2) is optimized for access by DMA. Normally large arrays & data buffers are placed in RAM2, to save the ultra-fast RAM1 for normal variables. The mapping of variables to memories is indicated at the variables declaration by compiler directive (such as DMAMEM for variable in RAM2 or FASTRUN for variable in RAM1, see here ). The Teensy Development Framework: teensyduino The Teensy can be programmed in many ways: The Arduino IDE software with the Teensyduino add-on is the primary programming environment for the Teensy; Visual Micro ; PlatformIO ; Makefiles: type make in directory $(arduino)/hardware/teensy/avr/cores/teensy4/ . In this course, we will be using the Arduino IDE with Teensyduino (but feel free to use any environment). In general, programming the Teensy amounts to compile an application to an executable ( main.elf usually) and then download the application on the Teensy which is connected through its USB interface to your PC. Installing teensyduino Teensyduino can be installed on Macintosh, Linux or Windows systems. The installation procedure can be found on the PJRC website (note that it slightly varies between platforms): https://www.pjrc.com/teensy/td_download.html On Linux, make sure to execute all the steps: Download Arduino (zip or AppImage) add repository https://www.pjrc.com/teensy/package_teensy_index.json as Additional boards manager URLs, and install it in Tools -> Board -> Board Manager Select Teensy 4.0 as board (Tools -> boards) Copy (as sudo) the Linux Udev Rule file (from here ) to /dev/udev/rules/ After this, clone the SON GitHub repository: https://github.com/inria-emeraude/son-ens Finally, copy the examples/teensy/libraries/mydsp (which contains a basic audio DSP library prepared for this course) folder from the course repo in $ARDUINOPATH/libraries where $ARDUINO can be seen in File->Preferences->Scketchbook location Hello World Write the following program in the programming interface: void setup() { Serial.begin(9600); } void loop() { Serial.println(\"Hello World!\"); delay(100); } Uploaded it on the Teensy (make sure that the right board and port are selected in Tools ). Open the serial debugger by clicking on the loop on the top right corner of the Arduino IDE. \"Hello World\" should be printed every 100ms. Running an Audio Program The course repository comes with a series of example audio programs \"making sound\" on the Teensy. Open `examples/teensy/projects/crazy-sine from your clone of https://github.com/inria-emeraude/son-ens in the Arduino IDE and run this program on your Teensy. Plug your headphones to the 3.5mm Jack connector on the Audio shield and you should hear a \"beautiful\" music.","title":" 1: Programming Environment Setup "},{"location":"lectures/setup/#programming-environment-setup","text":"The Teensy 4.0 microcontroller platform is used throughout this course. The goal of this first session is to teach you how to program the Teensy from your own computer.","title":"Programming Environment Setup"},{"location":"lectures/setup/#the-teensy-40-and-audio-shield","text":"When combined together, the Teensy 4.0 and its companion audio adapter (see figure below) constitute a great platform for embedded real-time audio signal processing providing a stereo audio input and output, interfacing possibilities with sensors, etc. The Teensy 4.0 (left) and the associated audio adapter board (right) The Teensy 4.0 is a microcontroller-based development board developed by PJRC. As an open source environment, tons of useful information can be found on their website: https://www.pjrc.com . The Teensy 4.0 is based on a powerful ARM Cortex-M7 microcontroller providing plenty of computational power (600 MHz with a Floating Point Unit - FPU) for real-time audio DSP (Digital Signal Processing) and many GPIOs (General Purpose Inputs and Outputs) for interfacing, etc. The CPU performances are many times faster than typical 32 bit microcontrollers. The FPU performs 32 bit float and 64 bit double precision math directly in hardware. DSP extension instructions accelerate signal processing, filters, Fourier transform, etc. The Teensy Audio library automatically makes use of these DSP instructions. Teensy performance from Core Benchmarks This pinout reference card comes with the Teensy 4.0 and should be part of your kit ( do not loose it! ). IMPORTANT: The pins are not 5V tolerant. Do not drive any digital pin higher than 3.3V. Teensy 4.0 pin map (from the PJRC webite) The Teensy 4.0 has a total of 40 input/output signal pins. 24 are easily accessible when used with a solderless breadboard. The available pins include general purpose IO (i.e., GPIO, digital or analog), as well as integrated serial protocols (i.e., I2C, I2S, CAN, SPI, and UART protocols) that are used to connect to other devices. In this course, we use the audio adaptor board provided by PJRC that integrates a low power stereo audio codec (NXP Semiconductors SGTL5000 codec) and a SD card reader. The Teensy audio adaptor (rev. D) The audio codec connects to Teensy using 7 signals (Yellow signal in pin map above) which are used by two protocols: I2C and I2S. This is a traditional configuration for audio codecs: the I2C (or I\u00b2C: Inter-Integrated Circuit) protocol is used to configure the codec (i.e., sample rate, input and output pins, etc.) and the I2S (or I\u00b2S: Inter-IC Sound) is used to transfer samples bit by bit in both direction (i.e., from and to the teensy). The I2C pins SDA and SCL are used to control the chip and adjust parameters. Audio data uses I2S signals, TX (to headphones and/or line out) and RX (from line in or mic), and 3 clocks, LRCLK (44.1 kHz), BCLK (1.41 MHz) and MCLK (11.29 MHz). All 3 clocks are created by Teensy which means that the SGTL5000 operates in \"slave mode.\" The schematics of the audio shield board, rev. D, can bee seen here and the schematic of the Teensy 4.0 can be seen at the end of this page: https://www.pjrc.com/store/teensy40.html . Of course, as they are both made by PJRC, they are designed to be compatible. The USB connector of the Teensy can support many serial communication from the host computer to the Teensy: (i.e., JTAG for flashing/programming, Serial UART, midi, mouse, etc. see Tools -> USB Type menu in arduino IDE). In this course, the USB connector is used to program the device (i.e., download binary code into flash memory) and ascii communication between the host and the Teensy (i.e., using UART/Serial communicatino protocol). In linux machines, when the teensy USB cable is connected, the serial port will appear as /dev/ttyACM0 . The Teensy 4.0 has 2 Mbyte of flash memory intended for storing your code. 1Mbyte of memory is available for execution (i.e., for variables and data storing during execution). Half of this memory (RAM1) is accessed as tightly coupled memory for maximum performance. The other half (RAM2) is optimized for access by DMA. Normally large arrays & data buffers are placed in RAM2, to save the ultra-fast RAM1 for normal variables. The mapping of variables to memories is indicated at the variables declaration by compiler directive (such as DMAMEM for variable in RAM2 or FASTRUN for variable in RAM1, see here ).","title":"The Teensy 4.0 and Audio Shield"},{"location":"lectures/setup/#the-teensy-development-framework-teensyduino","text":"The Teensy can be programmed in many ways: The Arduino IDE software with the Teensyduino add-on is the primary programming environment for the Teensy; Visual Micro ; PlatformIO ; Makefiles: type make in directory $(arduino)/hardware/teensy/avr/cores/teensy4/ . In this course, we will be using the Arduino IDE with Teensyduino (but feel free to use any environment). In general, programming the Teensy amounts to compile an application to an executable ( main.elf usually) and then download the application on the Teensy which is connected through its USB interface to your PC.","title":"The Teensy Development Framework: teensyduino"},{"location":"lectures/setup/#installing-teensyduino","text":"Teensyduino can be installed on Macintosh, Linux or Windows systems. The installation procedure can be found on the PJRC website (note that it slightly varies between platforms): https://www.pjrc.com/teensy/td_download.html On Linux, make sure to execute all the steps: Download Arduino (zip or AppImage) add repository https://www.pjrc.com/teensy/package_teensy_index.json as Additional boards manager URLs, and install it in Tools -> Board -> Board Manager Select Teensy 4.0 as board (Tools -> boards) Copy (as sudo) the Linux Udev Rule file (from here ) to /dev/udev/rules/ After this, clone the SON GitHub repository: https://github.com/inria-emeraude/son-ens Finally, copy the examples/teensy/libraries/mydsp (which contains a basic audio DSP library prepared for this course) folder from the course repo in $ARDUINOPATH/libraries where $ARDUINO can be seen in File->Preferences->Scketchbook location","title":"Installing teensyduino"},{"location":"lectures/setup/#hello-world","text":"Write the following program in the programming interface: void setup() { Serial.begin(9600); } void loop() { Serial.println(\"Hello World!\"); delay(100); } Uploaded it on the Teensy (make sure that the right board and port are selected in Tools ). Open the serial debugger by clicking on the loop on the top right corner of the Arduino IDE. \"Hello World\" should be printed every 100ms.","title":"Hello World"},{"location":"lectures/setup/#running-an-audio-program","text":"The course repository comes with a series of example audio programs \"making sound\" on the Teensy. Open `examples/teensy/projects/crazy-sine from your clone of https://github.com/inria-emeraude/son-ens in the Arduino IDE and run this program on your Teensy. Plug your headphones to the 3.5mm Jack connector on the Audio shield and you should hear a \"beautiful\" music.","title":"Running an Audio Program"},{"location":"lectures/useful/","text":"Some Other Usefull Things to Know This page gathered information that are useful to many of you (but not all of you probably), depending on the topic of your project. Many of your realization are prototyped in existing examples in the arduino IDE. Playing WAV file on the teensy with the SD-card If you want to play sound file, two possibilities: Use the USB-IN to stream audio from the computer (explained after) but you have no control on the file streamed from the teensy Store WAV files on the SD-Card and read them with the AudioPlaySdWav object. An example explaining how to use the SD-card reader can be found in file -> examples -> Audio ->> WaveFilePlayer Be careful to use the WAV file provided by teensy (https://www.pjrc.com/teensy/td_libs_AudioDataFiles.html) because some arbitraty WAV file may be not compatible with the on the AudioPlaySdWav player. Streaming Sound from through USB TODO microphone: how to solder it If your microphone has no JACK input port, you have to solder it. Microphone are stereo but the teensy input microphone is mono. The soldering of the JACK microphone is quite touchy. please get in touch with Teachers. TODO Using line in instead of microphone For some usage, for a guitar pedal effect for instance, the microphone (which is amplified) might be to sensitive for a \"Line\" signal such as the one getting out of a guitar. In reducing the microphone gain is not sufficient to eliminate saturation, you can use the \"Line IN\" port (port printed out below the audio shield) to solder another JACK adaptor. Use a MIDI keayboard or controller Many of you will want to use a midi controler (midi keyboard for instance) to control the teensy. A very simple way to do that is to use a virtual midi keyboard that will emulate a MIDI keybard on you computer and send midi signal to the teensy through the MIDI-USB interface. A very common virtual MIDI keyboard is vmpk (https://vmpk.sourceforge.io/), it works on Linux, Windows and Mac. In the arduino IDE, you will have to configure the USB-Type: tools -> USB-type -> \"MIDI+Serial\" There are many MIDI-USB example in the IDE, you can start with Files -> Examples -> Teensy -> USB_MIDI -> InputFunctionBasic Once the program running in the Teensy, you can launch the virtual keyboard and select Edit -> MIDI Connexions -> ouptut port connexions -> Teensy MIDI Be aware that, on some systems, you will have to restart the virtual keyboard each time you reprogram the Teensy. Midi controller through the PC, not directly. Once you have debugged your program with the virtual keyboard, you can plug a real (physical) MIDI keyboard to your computer and use your computer to transmit MIDI signal to the teensy with MIDI-USB. Note Frequency identification Many application will require to identify the fundamental frequency of a sound (i.e. the note of the sound). There are many ways to do that, one can use a FFT and identify the note on the spectrum. However, as we will see below, FFT is quite a heavy computationnal program and it is complicated to set-up. A simple way to do that in C++ is to use the Note Frequency example in the arduino IDE ( File -> Examples -> Audio -> Analysis -> NoteFrequency ) FAUST ef.transpose() function. The ef.transpose function is often useful, however, it uses too much memory to run on the teensy. You can still use this function but you have to copy the code of the transpose function (in the library github site, misceffect.lib file) in your faust program modify the line maxDelay = 65536; to maxDelay = 1024 Change ef.transpose by transpose Set the first parameter of transpose to 1000. Then you should be able to export your program to Tennsy. FFT or not FFT The Fourier transform is a powerful tool for many audio effect. However FFT is quite heavy computationnaly and difficult to set up so one must be aware that it will take some time to setup. FFT in Faust in not working . There is a FFT Faust program but it is called at each sample which is unrealistic (i.e. uses to much computing power). There is a AudioAnalyzeFFT1024 object in the teensy Audio library that Compute a 1024 point Fast Fourier Transform frequency analysis, with real value output. It is mainly used for spectral visualization. The example File -> Examples -> Audio -> Analysis -> FFT can be easily adapted to analyze the spectrum of the sound comming from the microphone (by default, it uses the line input). However, this object should be used in the loop of the .ino file. it will be too heavy to be used in the update function. Another point is that there is no example with a inverse FFT, there existe an inverse FFT function, but it cannor really by used to reconstruct the signal because it is not precise enough. Hence, at that point, FFT can be used for visualisation but not really to perform frequency domain sound transformations.","title":" Some Other Useful Things to Know "},{"location":"lectures/useful/#some-other-usefull-things-to-know","text":"This page gathered information that are useful to many of you (but not all of you probably), depending on the topic of your project. Many of your realization are prototyped in existing examples in the arduino IDE.","title":"Some Other Usefull Things to Know"},{"location":"lectures/useful/#playing-wav-file-on-the-teensy-with-the-sd-card","text":"If you want to play sound file, two possibilities: Use the USB-IN to stream audio from the computer (explained after) but you have no control on the file streamed from the teensy Store WAV files on the SD-Card and read them with the AudioPlaySdWav object. An example explaining how to use the SD-card reader can be found in file -> examples -> Audio ->> WaveFilePlayer Be careful to use the WAV file provided by teensy (https://www.pjrc.com/teensy/td_libs_AudioDataFiles.html) because some arbitraty WAV file may be not compatible with the on the AudioPlaySdWav player.","title":"Playing WAV file on the teensy with the SD-card"},{"location":"lectures/useful/#streaming-sound-from-through-usb","text":"TODO","title":"Streaming Sound from through USB"},{"location":"lectures/useful/#microphone-how-to-solder-it","text":"If your microphone has no JACK input port, you have to solder it. Microphone are stereo but the teensy input microphone is mono. The soldering of the JACK microphone is quite touchy. please get in touch with Teachers. TODO","title":"microphone: how to solder it"},{"location":"lectures/useful/#using-line-in-instead-of-microphone","text":"For some usage, for a guitar pedal effect for instance, the microphone (which is amplified) might be to sensitive for a \"Line\" signal such as the one getting out of a guitar. In reducing the microphone gain is not sufficient to eliminate saturation, you can use the \"Line IN\" port (port printed out below the audio shield) to solder another JACK adaptor.","title":"Using line in instead of microphone"},{"location":"lectures/useful/#use-a-midi-keayboard-or-controller","text":"Many of you will want to use a midi controler (midi keyboard for instance) to control the teensy. A very simple way to do that is to use a virtual midi keyboard that will emulate a MIDI keybard on you computer and send midi signal to the teensy through the MIDI-USB interface. A very common virtual MIDI keyboard is vmpk (https://vmpk.sourceforge.io/), it works on Linux, Windows and Mac. In the arduino IDE, you will have to configure the USB-Type: tools -> USB-type -> \"MIDI+Serial\" There are many MIDI-USB example in the IDE, you can start with Files -> Examples -> Teensy -> USB_MIDI -> InputFunctionBasic Once the program running in the Teensy, you can launch the virtual keyboard and select Edit -> MIDI Connexions -> ouptut port connexions -> Teensy MIDI Be aware that, on some systems, you will have to restart the virtual keyboard each time you reprogram the Teensy.","title":"Use a MIDI keayboard or controller"},{"location":"lectures/useful/#midi-controller-through-the-pc-not-directly","text":"Once you have debugged your program with the virtual keyboard, you can plug a real (physical) MIDI keyboard to your computer and use your computer to transmit MIDI signal to the teensy with MIDI-USB.","title":"Midi controller through the PC, not directly."},{"location":"lectures/useful/#note-frequency-identification","text":"Many application will require to identify the fundamental frequency of a sound (i.e. the note of the sound). There are many ways to do that, one can use a FFT and identify the note on the spectrum. However, as we will see below, FFT is quite a heavy computationnal program and it is complicated to set-up. A simple way to do that in C++ is to use the Note Frequency example in the arduino IDE ( File -> Examples -> Audio -> Analysis -> NoteFrequency )","title":"Note Frequency identification"},{"location":"lectures/useful/#faust-eftranspose-function","text":"The ef.transpose function is often useful, however, it uses too much memory to run on the teensy. You can still use this function but you have to copy the code of the transpose function (in the library github site, misceffect.lib file) in your faust program modify the line maxDelay = 65536; to maxDelay = 1024 Change ef.transpose by transpose Set the first parameter of transpose to 1000. Then you should be able to export your program to Tennsy.","title":"FAUST ef.transpose() function."},{"location":"lectures/useful/#fft-or-not-fft","text":"The Fourier transform is a powerful tool for many audio effect. However FFT is quite heavy computationnaly and difficult to set up so one must be aware that it will take some time to setup. FFT in Faust in not working . There is a FFT Faust program but it is called at each sample which is unrealistic (i.e. uses to much computing power). There is a AudioAnalyzeFFT1024 object in the teensy Audio library that Compute a 1024 point Fast Fourier Transform frequency analysis, with real value output. It is mainly used for spectral visualization. The example File -> Examples -> Audio -> Analysis -> FFT can be easily adapted to analyze the spectrum of the sound comming from the microphone (by default, it uses the line input). However, this object should be used in the loop of the .ino file. it will be too heavy to be used in the update function. Another point is that there is no example with a inverse FFT, there existe an inverse FFT function, but it cannor really by used to reconstruct the signal because it is not precise enough. Hence, at that point, FFT can be used for visualisation but not really to perform frequency domain sound transformations.","title":"FFT or not FFT"}]}